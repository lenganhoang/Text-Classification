{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lenganhoang/Text-Classification/blob/master/ALS_MoviesLen20M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZQDJKGUAQqn",
        "outputId": "769db642-a96a-4dfc-b07b-68978fec9fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19.6\n",
            "svmem(total=13617745920, available=12831154176, percent=5.8, used=524574720, free=11176697856, active=948625408, inactive=1299591168, buffers=107945984, cached=1808527360, shared=1220608, slab=127512576)\n",
            "5.8\n",
            "94.22377426762857\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "# gives a single float value\n",
        "print(psutil.cpu_percent())\n",
        "# gives an object with many fields\n",
        "print(psutil.virtual_memory())\n",
        "# you can convert that object to a dictionary print(dict(psutil.virtual_memory()._asdict()))\n",
        "# you can have the percentage of used RAM\n",
        "print(psutil.virtual_memory().percent)\n",
        "# you can calculate percentage of available memory\n",
        "print(psutil.virtual_memory().available * 100 / psutil.virtual_memory().total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XS6UHXPVAQqs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import gc\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gv8gXemwD3UQ",
        "outputId": "402ff6c8-38cb-4039-e5ec-ac97ec9f931c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Crhty6fiAQqu"
      },
      "outputs": [],
      "source": [
        "itemCol = 'movieId'\n",
        "userCol = 'userId'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DGX setup\n",
        "# fpath = \"./ml-20m\" \n",
        "\n",
        "#colab setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "fpath = \"/content/gdrive/MyDrive/ml_20M/ml-20m\""
      ],
      "metadata": {
        "id": "xOJJy441A0w0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc74d86b-a97b-4445-8ae6-9c27d67135dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data\n",
        "ratings = pd.read_csv(fpath+'/ratings.csv')\n",
        "\n",
        "ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "jQck8YHxCPJ1",
        "outputId": "f81c2d05-771b-4d78-9469-24aaaeee8dde"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          userId  movieId  rating   timestamp\n",
              "0              1        2     3.5  1112486027\n",
              "1              1       29     3.5  1112484676\n",
              "2              1       32     3.5  1112484819\n",
              "3              1       47     3.5  1112484727\n",
              "4              1       50     3.5  1112484580\n",
              "...          ...      ...     ...         ...\n",
              "20000258  138493    68954     4.5  1258126920\n",
              "20000259  138493    69526     4.5  1259865108\n",
              "20000260  138493    69644     3.0  1260209457\n",
              "20000261  138493    70286     5.0  1258126944\n",
              "20000262  138493    71619     2.5  1255811136\n",
              "\n",
              "[20000263 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-719906dc-72d8-4d51-b2ed-8a63c93a6796\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112486027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000258</th>\n",
              "      <td>138493</td>\n",
              "      <td>68954</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1258126920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000259</th>\n",
              "      <td>138493</td>\n",
              "      <td>69526</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1259865108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000260</th>\n",
              "      <td>138493</td>\n",
              "      <td>69644</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1260209457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000261</th>\n",
              "      <td>138493</td>\n",
              "      <td>70286</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1258126944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000262</th>\n",
              "      <td>138493</td>\n",
              "      <td>71619</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1255811136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000263 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-719906dc-72d8-4d51-b2ed-8a63c93a6796')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-719906dc-72d8-4d51-b2ed-8a63c93a6796 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-719906dc-72d8-4d51-b2ed-8a63c93a6796');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NaQ801dq3uO",
        "outputId": "1a46f4d0-8a05-4326-dc3f-2cea9393daf1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=89bd98c4ffcad3b7cdbce32c61e150a640181b791e4cc8800bbfcc76aa22b485\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pyspark\n",
        "from pyspark.ml.recommendation import ALS\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField\n",
        "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
        "import warnings"
      ],
      "metadata": {
        "id": "Jg61Og0uql3U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_USER_COL = \"userId\"\n",
        "DEFAULT_ITEM_COL = \"movieId\"\n",
        "DEFAULT_RATING_COL = \"rating\"\n",
        "DEFAULT_LABEL_COL = \"label\"\n",
        "DEFAULT_TITLE_COL = \"title\"\n",
        "DEFAULT_GENRE_COL = \"genre\"\n",
        "DEFAULT_RELEVANCE_COL = \"relevance\"\n",
        "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
        "DEFAULT_PREDICTION_COL = \"prediction\"\n",
        "DEFAULT_SIMILARITY_COL = \"sim\"\n",
        "DEFAULT_ITEM_FEATURES_COL = \"features\"\n",
        "DEFAULT_ITEM_SIM_MEASURE = \"item_cooccurrence_count\""
      ],
      "metadata": {
        "id": "usdIt9msrNSp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from pyspark.sql import functions as F, Window\n",
        "    from pyspark.storagelevel import StorageLevel\n",
        "except ImportError:\n",
        "    pass  # skip this import if we are in pure python environment\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "try:\n",
        "    from pyspark.sql import functions as F, Window\n",
        "except ImportError:\n",
        "    pass  # so the environment without spark doesn't break\n",
        "\n",
        "\n",
        "def process_split_ratio(ratio):\n",
        "    \"\"\"Generate split ratio lists.\n",
        "    Args:\n",
        "        ratio (float or list): a float number that indicates split ratio or a list of float\n",
        "        numbers that indicate split ratios (if it is a multi-split).\n",
        "    Returns:\n",
        "        tuple:\n",
        "        - bool: A boolean variable multi that indicates if the splitting is multi or single.\n",
        "        - list: A list of normalized split ratios.\n",
        "    \"\"\"\n",
        "    if isinstance(ratio, float):\n",
        "        if ratio <= 0 or ratio >= 1:\n",
        "            raise ValueError(\"Split ratio has to be between 0 and 1\")\n",
        "\n",
        "        multi = False\n",
        "    elif isinstance(ratio, list):\n",
        "        if any([x <= 0 for x in ratio]):\n",
        "            raise ValueError(\n",
        "                \"All split ratios in the ratio list should be larger than 0.\"\n",
        "            )\n",
        "\n",
        "        # normalize split ratios if they are not summed to 1\n",
        "        if math.fsum(ratio) != 1.0:\n",
        "            ratio = [x / math.fsum(ratio) for x in ratio]\n",
        "\n",
        "        multi = True\n",
        "    else:\n",
        "        raise TypeError(\"Split ratio should be either float or a list of floats.\")\n",
        "\n",
        "    return multi, ratio\n",
        "\n",
        "def _get_column_name(name, col_user, col_item):\n",
        "    if name == \"user\":\n",
        "        return col_user\n",
        "    elif name == \"item\":\n",
        "        return col_item\n",
        "    else:\n",
        "        raise ValueError(\"name should be either 'user' or 'item'.\")\n",
        "\n",
        "\n",
        "def min_rating_filter_spark(\n",
        "    data,\n",
        "    min_rating=1,\n",
        "    filter_by=\"user\",\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "):\n",
        "    \"\"\"Filter rating DataFrame for each user with minimum rating.\n",
        "    Filter rating data frame with minimum number of ratings for user/item is usually useful to\n",
        "    generate a new data frame with warm user/item. The warmth is defined by min_rating argument. For\n",
        "    example, a user is called warm if he has rated at least 4 items.\n",
        "    Args:\n",
        "        data (pyspark.sql.DataFrame): DataFrame of user-item tuples. Columns of user and item\n",
        "            should be present in the DataFrame while other columns like rating,\n",
        "            timestamp, etc. can be optional.\n",
        "        min_rating (int): minimum number of ratings for user or item.\n",
        "        filter_by (str): either \"user\" or \"item\", depending on which of the two is to\n",
        "            filter with min_rating.\n",
        "        col_user (str): column name of user ID.\n",
        "        col_item (str): column name of item ID.\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: DataFrame with at least columns of user and item that has been filtered by the given specifications.\n",
        "    \"\"\"\n",
        "\n",
        "    split_by_column = _get_column_name(filter_by, col_user, col_item)\n",
        "\n",
        "    if min_rating < 1:\n",
        "        raise ValueError(\"min_rating should be integer and larger than or equal to 1.\")\n",
        "\n",
        "    if min_rating > 1:\n",
        "        window = Window.partitionBy(split_by_column)\n",
        "        data = (\n",
        "            data.withColumn(\"_count\", F.count(split_by_column).over(window))\n",
        "            .where(F.col(\"_count\") >= min_rating)\n",
        "            .drop(\"_count\")\n",
        "        )\n",
        "\n",
        "    return data\n",
        "\n",
        "def spark_random_split(data, ratio=0.75, seed=42):\n",
        "    \"\"\"Spark random splitter.\n",
        "    Randomly split the data into several splits.\n",
        "    Args:\n",
        "        data (pyspark.sql.DataFrame): Spark DataFrame to be split.\n",
        "        ratio (float or list): Ratio for splitting data. If it is a single float number\n",
        "            it splits data into two halves and the ratio argument indicates the ratio of\n",
        "            training data set; if it is a list of float numbers, the splitter splits\n",
        "            data into several portions corresponding to the split ratios. If a list\n",
        "            is provided and the ratios are not summed to 1, they will be normalized.\n",
        "        seed (int): Seed.\n",
        "    Returns:\n",
        "        list: Splits of the input data as pyspark.sql.DataFrame.\n",
        "    \"\"\"\n",
        "    multi_split, ratio = process_split_ratio(ratio)\n",
        "\n",
        "    if multi_split:\n",
        "        return data.randomSplit(ratio, seed=seed)\n",
        "    else:\n",
        "        return data.randomSplit([ratio, 1 - ratio], seed=seed)\n",
        "\n",
        "\n",
        "def _do_stratification_spark(\n",
        "    data,\n",
        "    ratio=0.75,\n",
        "    min_rating=1,\n",
        "    filter_by=\"user\",\n",
        "    is_partitioned=True,\n",
        "    is_random=True,\n",
        "    seed=42,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_timestamp=DEFAULT_TIMESTAMP_COL,\n",
        "):\n",
        "    \"\"\"Helper function to perform stratified splits.\n",
        "    This function splits data in a stratified manner. That is, the same values for the\n",
        "    filter_by column are retained in each split, but the corresponding set of entries\n",
        "    are divided according to the ratio provided.\n",
        "    Args:\n",
        "        data (pyspark.sql.DataFrame): Spark DataFrame to be split.\n",
        "        ratio (float or list): Ratio for splitting data. If it is a single float number\n",
        "            it splits data into two sets and the ratio argument indicates the ratio of\n",
        "            training data set; if it is a list of float numbers, the splitter splits\n",
        "            data into several portions corresponding to the split ratios. If a list is\n",
        "            provided and the ratios are not summed to 1, they will be normalized.\n",
        "        min_rating (int): minimum number of ratings for user or item.\n",
        "        filter_by (str): either \"user\" or \"item\", depending on which of the two is to filter\n",
        "            with min_rating.\n",
        "        is_partitioned (bool): flag to partition data by filter_by column\n",
        "        is_random (bool): flag to make split randomly or use timestamp column\n",
        "        seed (int): Seed.\n",
        "        col_user (str): column name of user IDs.\n",
        "        col_item (str): column name of item IDs.\n",
        "        col_timestamp (str): column name of timestamps.\n",
        "    Args:\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    # A few preliminary checks.\n",
        "    if filter_by not in [\"user\", \"item\"]:\n",
        "        raise ValueError(\"filter_by should be either 'user' or 'item'.\")\n",
        "\n",
        "    if min_rating < 1:\n",
        "        raise ValueError(\"min_rating should be integer and larger than or equal to 1.\")\n",
        "\n",
        "    if col_user not in data.columns:\n",
        "        raise ValueError(\"Schema of data not valid. Missing User Col\")\n",
        "\n",
        "    if col_item not in data.columns:\n",
        "        raise ValueError(\"Schema of data not valid. Missing Item Col\")\n",
        "\n",
        "    if not is_random:\n",
        "        if col_timestamp not in data.columns:\n",
        "            raise ValueError(\"Schema of data not valid. Missing Timestamp Col\")\n",
        "\n",
        "    if min_rating > 1:\n",
        "        data = min_rating_filter_spark(\n",
        "            data=data,\n",
        "            min_rating=min_rating,\n",
        "            filter_by=filter_by,\n",
        "            col_user=col_user,\n",
        "            col_item=col_item,\n",
        "        )\n",
        "\n",
        "    split_by = col_user if filter_by == \"user\" else col_item\n",
        "    partition_by = split_by if is_partitioned else []\n",
        "\n",
        "    col_random = \"_random\"\n",
        "    if is_random:\n",
        "        data = data.withColumn(col_random, F.rand(seed=seed))\n",
        "        order_by = F.col(col_random)\n",
        "    else:\n",
        "        order_by = F.col(col_timestamp)\n",
        "\n",
        "    window_count = Window.partitionBy(partition_by)\n",
        "    window_spec = Window.partitionBy(partition_by).orderBy(order_by)\n",
        "\n",
        "    data = (\n",
        "        data.withColumn(\"_count\", F.count(split_by).over(window_count))\n",
        "        .withColumn(\"_rank\", F.row_number().over(window_spec) / F.col(\"_count\"))\n",
        "        .drop(\"_count\", col_random)\n",
        "    )\n",
        "    # Persist to avoid duplicate rows in splits caused by lazy evaluation\n",
        "    data.persist(StorageLevel.MEMORY_AND_DISK_2).count()\n",
        "\n",
        "    multi_split, ratio = process_split_ratio(ratio)\n",
        "    ratio = ratio if multi_split else [ratio, 1 - ratio]\n",
        "\n",
        "    splits = []\n",
        "    prev_split = None\n",
        "    for split in np.cumsum(ratio):\n",
        "        condition = F.col(\"_rank\") <= split\n",
        "        if prev_split is not None:\n",
        "            condition &= F.col(\"_rank\") > prev_split\n",
        "        splits.append(data.filter(condition).drop(\"_rank\"))\n",
        "        prev_split = split\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "def spark_chrono_split(\n",
        "    data,\n",
        "    ratio=0.75,\n",
        "    min_rating=1,\n",
        "    filter_by=\"user\",\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_timestamp=DEFAULT_TIMESTAMP_COL,\n",
        "    no_partition=False,\n",
        "):\n",
        "    \"\"\"Spark chronological splitter.\n",
        "    This function splits data in a chronological manner. That is, for each user / item, the\n",
        "    split function takes proportions of ratings which is specified by the split ratio(s).\n",
        "    The split is stratified.\n",
        "    Args:\n",
        "        data (pyspark.sql.DataFrame): Spark DataFrame to be split.\n",
        "        ratio (float or list): Ratio for splitting data. If it is a single float number\n",
        "            it splits data into two sets and the ratio argument indicates the ratio of\n",
        "            training data set; if it is a list of float numbers, the splitter splits\n",
        "            data into several portions corresponding to the split ratios. If a list is\n",
        "            provided and the ratios are not summed to 1, they will be normalized.\n",
        "        min_rating (int): minimum number of ratings for user or item.\n",
        "        filter_by (str): either \"user\" or \"item\", depending on which of the two is to filter\n",
        "            with min_rating.\n",
        "        col_user (str): column name of user IDs.\n",
        "        col_item (str): column name of item IDs.\n",
        "        col_timestamp (str): column name of timestamps.\n",
        "        no_partition (bool): set to enable more accurate and less efficient splitting.\n",
        "    Returns:\n",
        "        list: Splits of the input data as pyspark.sql.DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    return _do_stratification_spark(\n",
        "        data=data,\n",
        "        ratio=ratio,\n",
        "        min_rating=min_rating,\n",
        "        filter_by=filter_by,\n",
        "        is_random=False,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_timestamp=col_timestamp,\n",
        "    )\n",
        "\n",
        "\n",
        "def spark_stratified_split(\n",
        "    data,\n",
        "    ratio=0.75,\n",
        "    min_rating=1,\n",
        "    filter_by=\"user\",\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"Spark stratified splitter.\n",
        "    For each user / item, the split function takes proportions of ratings which is\n",
        "    specified by the split ratio(s). The split is stratified.\n",
        "    Args:\n",
        "        data (pyspark.sql.DataFrame): Spark DataFrame to be split.\n",
        "        ratio (float or list): Ratio for splitting data. If it is a single float number\n",
        "            it splits data into two halves and the ratio argument indicates the ratio of\n",
        "            training data set; if it is a list of float numbers, the splitter splits\n",
        "            data into several portions corresponding to the split ratios. If a list is\n",
        "            provided and the ratios are not summed to 1, they will be normalized.\n",
        "            Earlier indexed splits will have earlier times\n",
        "            (e.g. the latest time per user or item in split[0] <= the earliest time per user or item in split[1])\n",
        "        seed (int): Seed.\n",
        "        min_rating (int): minimum number of ratings for user or item.\n",
        "        filter_by (str): either \"user\" or \"item\", depending on which of the two is to filter\n",
        "            with min_rating.\n",
        "        col_user (str): column name of user IDs.\n",
        "        col_item (str): column name of item IDs.\n",
        "    Returns:\n",
        "        list: Splits of the input data as pyspark.sql.DataFrame.\n",
        "    \"\"\"\n",
        "    return _do_stratification_spark(\n",
        "        data=data,\n",
        "        ratio=ratio,\n",
        "        min_rating=min_rating,\n",
        "        filter_by=filter_by,\n",
        "        seed=seed,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "    )\n",
        "\n",
        "\n",
        "def spark_timestamp_split(\n",
        "    data,\n",
        "    ratio=0.75,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_timestamp=DEFAULT_TIMESTAMP_COL,\n",
        "):\n",
        "    \"\"\"Spark timestamp based splitter.\n",
        "    The splitter splits the data into sets by timestamps without stratification on either user or item.\n",
        "    The ratios are applied on the timestamp column which is divided accordingly into several partitions.\n",
        "    Args:\n",
        "        data (pyspark.sql.DataFrame): Spark DataFrame to be split.\n",
        "        ratio (float or list): Ratio for splitting data. If it is a single float number\n",
        "            it splits data into two sets and the ratio argument indicates the ratio of\n",
        "            training data set; if it is a list of float numbers, the splitter splits\n",
        "            data into several portions corresponding to the split ratios. If a list is\n",
        "            provided and the ratios are not summed to 1, they will be normalized.\n",
        "            Earlier indexed splits will have earlier times\n",
        "            (e.g. the latest time in split[0] <= the earliest time in split[1])\n",
        "        col_user (str): column name of user IDs.\n",
        "        col_item (str): column name of item IDs.\n",
        "        col_timestamp (str): column name of timestamps. Float number represented in\n",
        "        seconds since Epoch.\n",
        "    Returns:\n",
        "        list: Splits of the input data as pyspark.sql.DataFrame.\n",
        "    \"\"\"\n",
        "    return _do_stratification_spark(\n",
        "        data=data,\n",
        "        ratio=ratio,\n",
        "        is_random=False,\n",
        "        is_partitioned=False,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_timestamp=col_timestamp,\n",
        "    )"
      ],
      "metadata": {
        "id": "S5DISzPTrIVc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top k items to recommend\n",
        "TOP_K = 50\n",
        "\n",
        "# Column names for the dataset\n",
        "COL_USER = \"userId\"\n",
        "COL_ITEM = \"movieId\"\n",
        "COL_RATING = \"rating\"\n",
        "COL_TIMESTAMP = \"timestamp\""
      ],
      "metadata": {
        "id": "TgTZJKnRsCZa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
        "# set up a giant single executor with many threads and specify memory cap\n",
        "import os\n",
        "\n",
        "\n",
        "try:\n",
        "    from pyspark.sql import SparkSession  # noqa: F401\n",
        "except ImportError:\n",
        "    pass  # skip this import if we are in pure python environment\n",
        "\n",
        "MMLSPARK_PACKAGE = \"com.microsoft.azure:synapseml_2.12:0.9.5\"\n",
        "MMLSPARK_REPO = \"https://mmlspark.azureedge.net/maven\"\n",
        "# We support Spark v3, but in case you wish to use v2, set\n",
        "# MMLSPARK_PACKAGE = \"com.microsoft.ml.spark:mmlspark_2.11:0.18.1\"\n",
        "# MMLSPARK_REPO = \"https://mvnrepository.com/artifact\"\n",
        "\n",
        "\n",
        "def start_or_get_spark(\n",
        "    app_name=\"Sample\",\n",
        "    url=\"local[*]\",\n",
        "    memory=\"10g\",\n",
        "    config=None,\n",
        "    packages=None,\n",
        "    jars=None,\n",
        "    repositories=None,\n",
        "):\n",
        "    \"\"\"Start Spark if not started\n",
        "    Args:\n",
        "        app_name (str): set name of the application\n",
        "        url (str): URL for spark master\n",
        "        memory (str): size of memory for spark driver\n",
        "        config (dict): dictionary of configuration options\n",
        "        packages (list): list of packages to install\n",
        "        jars (list): list of jar files to add\n",
        "        repositories (list): list of maven repositories\n",
        "    Returns:\n",
        "        object: Spark context.\n",
        "    \"\"\"\n",
        "\n",
        "    submit_args = \"\"\n",
        "    if packages is not None:\n",
        "        submit_args = \"--packages {} \".format(\",\".join(packages))\n",
        "    if jars is not None:\n",
        "        submit_args += \"--jars {} \".format(\",\".join(jars))\n",
        "    if repositories is not None:\n",
        "        submit_args += \"--repositories {}\".format(\",\".join(repositories))\n",
        "    if submit_args:\n",
        "        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"{} pyspark-shell\".format(submit_args)\n",
        "\n",
        "    spark_opts = [\n",
        "        'SparkSession.builder.appName(\"{}\")'.format(app_name),\n",
        "        'master(\"{}\")'.format(url),\n",
        "    ]\n",
        "\n",
        "    if config is not None:\n",
        "        for key, raw_value in config.items():\n",
        "            value = (\n",
        "                '\"{}\"'.format(raw_value) if isinstance(raw_value, str) else raw_value\n",
        "            )\n",
        "            spark_opts.append('config(\"{key}\", {value})'.format(key=key, value=value))\n",
        "\n",
        "    if config is None or \"spark.driver.memory\" not in config:\n",
        "        spark_opts.append('config(\"spark.driver.memory\", \"{}\")'.format(memory))\n",
        "\n",
        "    spark_opts.append(\"getOrCreate()\")\n",
        "    return eval(\".\".join(spark_opts))\n",
        "spark = start_or_get_spark(\"ALS PySpark\", memory=\"16g\")\n",
        "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")"
      ],
      "metadata": {
        "id": "LsI0m-LBv_LS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "data = spark.read.option(\"header\",True).csv(fpath+\"/ratings.csv\")\n",
        "data = data.withColumn(\"userId\",F.col(\"userId\").cast(\"integer\"))\n",
        "data = data.withColumn(\"movieId\",F.col(\"movieId\").cast('integer'))\n",
        "data = data.withColumn(\"rating\",F.col(\"rating\").cast('double'))\n",
        "data = data.withColumn(\"timestamp\",F.col(\"timestamp\").cast('integer'))\n",
        "#data.show()"
      ],
      "metadata": {
        "id": "8Fuv4_URsc6O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = spark_random_split(data, ratio=0.75, seed=123)\n",
        "print (\"N train\", train.cache().count())\n",
        "print (\"N test\", test.cache().count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtox66-TwZVV",
        "outputId": "22459779-a24b-469c-c0f5-31256e96345c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N train 14997077\n",
            "N test 5003186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header = {\n",
        "    \"userCol\": COL_USER,\n",
        "    \"itemCol\": COL_ITEM,\n",
        "    \"ratingCol\": COL_RATING,\n",
        "}\n",
        "\n",
        "\n",
        "als = ALS(\n",
        "    rank=50,\n",
        "    maxIter=15,\n",
        "    implicitPrefs=False,\n",
        "    regParam=0.05,\n",
        "    coldStartStrategy='drop',\n",
        "    nonnegative=False,\n",
        "    seed=42,\n",
        "    **header\n",
        ")"
      ],
      "metadata": {
        "id": "Ho4yW8Efw8o7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer\n",
        "\n",
        "\n",
        "class Timer(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self._timer = default_timer\n",
        "        self._interval = 0\n",
        "        self.running = False\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.stop()\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"{:0.4f}\".format(self.interval)\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.init = self._timer()\n",
        "        self.running = True\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer. Calculate the interval in seconds.\"\"\"\n",
        "        self.end = self._timer()\n",
        "        try:\n",
        "            self._interval = self.end - self.init\n",
        "            self.running = False\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Timer has not been initialized: use start() or the contextual form with Timer() as t:\"\n",
        "            )\n",
        "\n",
        "    @property\n",
        "    def interval(self):\n",
        "        \"\"\"Get time interval in seconds.\n",
        "        Returns:\n",
        "            float: Seconds.\n",
        "        \"\"\"\n",
        "        if self.running:\n",
        "            raise ValueError(\"Timer has not been stopped, please use stop().\")\n",
        "        else:\n",
        "            return self._interval"
      ],
      "metadata": {
        "id": "_PDQE2Ipyo0n"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with Timer() as train_time:\n",
        "    model = als.fit(train)\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHXvoAEYxGRl",
        "outputId": "f188720c-9e2e-40cb-b8d5-214d18275cc1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 3296.674539312 seconds for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = fpath + \"/als_model\"\n",
        "model.save(model_path)"
      ],
      "metadata": {
        "id": "xWdovSunR8qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with Timer() as test_time:\n",
        "\n",
        "    # Get the cross join of all user-item pairs and score them.\n",
        "    users = train.select(COL_USER).distinct().limit(100)\n",
        "    items = train.select(COL_ITEM).distinct()\n",
        "    user_item = users.crossJoin(items)\n",
        "    dfs_pred = model.transform(user_item)\n",
        "\n",
        "    # Remove seen items.\n",
        "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
        "        train.alias(\"train\"),\n",
        "        (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_ITEM] == train[COL_ITEM]),\n",
        "        how='outer'\n",
        "    )\n",
        "\n",
        "    top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_RATING}\"].isNull()) \\\n",
        "        .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
        "\n",
        "    # In Spark, transformations are lazy evaluation\n",
        "    # Use an action to force execute and measure the test time \n",
        "    #top_all.cache().count()\n",
        "\n",
        "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOmuF0bSRfL-",
        "outputId": "c2f631f1-3765-4c45-bb52-eb7f4722d6df"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 0.1855022720001216 seconds for prediction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_all.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QmqRPesy_QE",
        "outputId": "268b20d3-e336-42fb-9d71-1b659103b72e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+----------+\n",
            "|userId|movieId|prediction|\n",
            "+------+-------+----------+\n",
            "|   148|      7| 4.0048413|\n",
            "|   148|     12| 1.6690178|\n",
            "|   148|     16| 2.6183758|\n",
            "|   148|     23| 2.5193338|\n",
            "|   148|     24| 2.8611636|\n",
            "|   148|     28|  4.146655|\n",
            "|   148|     29| 2.7157655|\n",
            "|   148|     30| 2.8979905|\n",
            "|   148|     33| 3.2868037|\n",
            "|   148|     35|  2.659297|\n",
            "|   148|     36|  3.667456|\n",
            "|   148|     41|  3.213024|\n",
            "|   148|     43| 2.8771875|\n",
            "|   148|     53| 3.2202995|\n",
            "|   148|     54| 3.0606828|\n",
            "|   148|     57| 2.9996724|\n",
            "|   148|     59| 2.4303079|\n",
            "|   148|     62| 4.0918036|\n",
            "|   148|     63| 1.2289059|\n",
            "|   148|     66| 0.9142319|\n",
            "+------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "try:\n",
        "    from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
        "    from pyspark.sql import Window, DataFrame\n",
        "    from pyspark.sql.functions import col, row_number, expr\n",
        "    from pyspark.sql.functions import udf\n",
        "    import pyspark.sql.functions as F\n",
        "    from pyspark.sql.types import IntegerType, DoubleType, StructType, StructField\n",
        "    from pyspark.ml.linalg import VectorUDT\n",
        "except ImportError:\n",
        "    pass  # skip this import if we are in pure python environment\n",
        "\n",
        "\n",
        "class SparkRatingEvaluation:\n",
        "    \"\"\"Spark Rating Evaluator\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        rating_true,\n",
        "        rating_pred,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        col_rating=DEFAULT_RATING_COL,\n",
        "        col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    ):\n",
        "        \"\"\"Initializer.\n",
        "        This is the Spark version of rating metrics evaluator.\n",
        "        The methods of this class, calculate rating metrics such as root mean squared error, mean absolute error,\n",
        "        R squared, and explained variance.\n",
        "        Args:\n",
        "            rating_true (pyspark.sql.DataFrame): True labels.\n",
        "            rating_pred (pyspark.sql.DataFrame): Predicted labels.\n",
        "            col_user (str): column name for user.\n",
        "            col_item (str): column name for item.\n",
        "            col_rating (str): column name for rating.\n",
        "            col_prediction (str): column name for prediction.\n",
        "        \"\"\"\n",
        "        self.rating_true = rating_true\n",
        "        self.rating_pred = rating_pred\n",
        "        self.col_user = col_user\n",
        "        self.col_item = col_item\n",
        "        self.col_rating = col_rating\n",
        "        self.col_prediction = col_prediction\n",
        "\n",
        "        # Check if inputs are Spark DataFrames.\n",
        "        if not isinstance(self.rating_true, DataFrame):\n",
        "            raise TypeError(\n",
        "                \"rating_true should be but is not a Spark DataFrame\"\n",
        "            )  # pragma : No Cover\n",
        "\n",
        "        if not isinstance(self.rating_pred, DataFrame):\n",
        "            raise TypeError(\n",
        "                \"rating_pred should be but is not a Spark DataFrame\"\n",
        "            )  # pragma : No Cover\n",
        "\n",
        "        # Check if columns exist.\n",
        "        true_columns = self.rating_true.columns\n",
        "        pred_columns = self.rating_pred.columns\n",
        "\n",
        "        if rating_true.count() == 0:\n",
        "            raise ValueError(\"Empty input dataframe\")\n",
        "        if rating_pred.count() == 0:\n",
        "            raise ValueError(\"Empty input dataframe\")\n",
        "\n",
        "        if self.col_user not in true_columns:\n",
        "            raise ValueError(\"Schema of rating_true not valid. Missing User Col\")\n",
        "        if self.col_item not in true_columns:\n",
        "            raise ValueError(\"Schema of rating_true not valid. Missing Item Col\")\n",
        "        if self.col_rating not in true_columns:\n",
        "            raise ValueError(\"Schema of rating_true not valid. Missing Rating Col\")\n",
        "\n",
        "        if self.col_user not in pred_columns:\n",
        "            raise ValueError(\n",
        "                \"Schema of rating_pred not valid. Missing User Col\"\n",
        "            )  # pragma : No Cover\n",
        "        if self.col_item not in pred_columns:\n",
        "            raise ValueError(\n",
        "                \"Schema of rating_pred not valid. Missing Item Col\"\n",
        "            )  # pragma : No Cover\n",
        "        if self.col_prediction not in pred_columns:\n",
        "            raise ValueError(\"Schema of rating_pred not valid. Missing Prediction Col\")\n",
        "\n",
        "        self.rating_true = self.rating_true.select(\n",
        "            col(self.col_user),\n",
        "            col(self.col_item),\n",
        "            col(self.col_rating).cast(\"double\").alias(\"label\"),\n",
        "        )\n",
        "        self.rating_pred = self.rating_pred.select(\n",
        "            col(self.col_user),\n",
        "            col(self.col_item),\n",
        "            col(self.col_prediction).cast(\"double\").alias(\"prediction\"),\n",
        "        )\n",
        "\n",
        "        self.y_pred_true = (\n",
        "            self.rating_true.join(\n",
        "                self.rating_pred, [self.col_user, self.col_item], \"inner\"\n",
        "            )\n",
        "            .drop(self.col_user)\n",
        "            .drop(self.col_item)\n",
        "        )\n",
        "\n",
        "        self.metrics = RegressionMetrics(\n",
        "            self.y_pred_true.rdd.map(lambda x: (x.prediction, x.label))\n",
        "        )\n",
        "\n",
        "    def rmse(self):\n",
        "        \"\"\"Calculate Root Mean Squared Error.\n",
        "        Returns:\n",
        "            float: Root mean squared error.\n",
        "        \"\"\"\n",
        "        return self.metrics.rootMeanSquaredError\n",
        "\n",
        "    def mae(self):\n",
        "        \"\"\"Calculate Mean Absolute Error.\n",
        "        Returns:\n",
        "            float: Mean Absolute Error.\n",
        "        \"\"\"\n",
        "        return self.metrics.meanAbsoluteError\n",
        "\n",
        "    def rsquared(self):\n",
        "        \"\"\"Calculate R squared.\n",
        "        Returns:\n",
        "            float: R squared.\n",
        "        \"\"\"\n",
        "        return self.metrics.r2\n",
        "\n",
        "    def exp_var(self):\n",
        "        \"\"\"Calculate explained variance.\n",
        "        .. note::\n",
        "           Spark MLLib's implementation is buggy (can lead to values > 1), hence we use var().\n",
        "        Returns:\n",
        "            float: Explained variance (min=0, max=1).\n",
        "        \"\"\"\n",
        "        var1 = self.y_pred_true.selectExpr(\"variance(label - prediction)\").collect()[0][\n",
        "            0\n",
        "        ]\n",
        "        var2 = self.y_pred_true.selectExpr(\"variance(label)\").collect()[0][0]\n",
        "        # numpy divide is more tolerant to var2 being zero\n",
        "        return 1 - np.divide(var1, var2)\n",
        "\n",
        "\n",
        "class SparkRankingEvaluation:\n",
        "    \"\"\"Spark Ranking Evaluator\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        rating_true,\n",
        "        rating_pred,\n",
        "        k=50,\n",
        "        relevancy_method=\"top_k\",\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        col_rating=DEFAULT_RATING_COL,\n",
        "        col_prediction=DEFAULT_PREDICTION_COL,\n",
        "        threshold=50,\n",
        "    ):\n",
        "        \"\"\"Initialization.\n",
        "        This is the Spark version of ranking metrics evaluator.\n",
        "        The methods of this class, calculate ranking metrics such as precision@k, recall@k, ndcg@k, and mean average\n",
        "        precision.\n",
        "        The implementations of precision@k, ndcg@k, and mean average precision are referenced from Spark MLlib, which\n",
        "        can be found at `here <https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems>`_.\n",
        "        Args:\n",
        "            rating_true (pyspark.sql.DataFrame): DataFrame of true rating data (in the\n",
        "                format of customerID-itemID-rating tuple).\n",
        "            rating_pred (pyspark.sql.DataFrame): DataFrame of predicted rating data (in\n",
        "                the format of customerID-itemID-rating tuple).\n",
        "            col_user (str): column name for user.\n",
        "            col_item (str): column name for item.\n",
        "            col_rating (str): column name for rating.\n",
        "            col_prediction (str): column name for prediction.\n",
        "            k (int): number of items to recommend to each user.\n",
        "            relevancy_method (str): method for determining relevant items. Possible\n",
        "                values are \"top_k\", \"by_time_stamp\", and \"by_threshold\".\n",
        "            threshold (float): threshold for determining the relevant recommended items.\n",
        "                This is used for the case that predicted ratings follow a known\n",
        "                distribution. NOTE: this option is only activated if relevancy_method is\n",
        "                set to \"by_threshold\".\n",
        "        \"\"\"\n",
        "        self.rating_true = rating_true\n",
        "        self.rating_pred = rating_pred\n",
        "        self.col_user = col_user\n",
        "        self.col_item = col_item\n",
        "        self.col_rating = col_rating\n",
        "        self.col_prediction = col_prediction\n",
        "        self.threshold = threshold\n",
        "\n",
        "        # Check if inputs are Spark DataFrames.\n",
        "        if not isinstance(self.rating_true, DataFrame):\n",
        "            raise TypeError(\n",
        "                \"rating_true should be but is not a Spark DataFrame\"\n",
        "            )  # pragma : No Cover\n",
        "\n",
        "        if not isinstance(self.rating_pred, DataFrame):\n",
        "            raise TypeError(\n",
        "                \"rating_pred should be but is not a Spark DataFrame\"\n",
        "            )  # pragma : No Cover\n",
        "\n",
        "        # Check if columns exist.\n",
        "        true_columns = self.rating_true.columns\n",
        "        pred_columns = self.rating_pred.columns\n",
        "\n",
        "        if self.col_user not in true_columns:\n",
        "            raise ValueError(\n",
        "                \"Schema of rating_true not valid. Missing User Col: \"\n",
        "                + str(true_columns)\n",
        "            )\n",
        "        if self.col_item not in true_columns:\n",
        "            raise ValueError(\"Schema of rating_true not valid. Missing Item Col\")\n",
        "        if self.col_rating not in true_columns:\n",
        "            raise ValueError(\"Schema of rating_true not valid. Missing Rating Col\")\n",
        "\n",
        "        if self.col_user not in pred_columns:\n",
        "            raise ValueError(\n",
        "                \"Schema of rating_pred not valid. Missing User Col\"\n",
        "            )  # pragma : No Cover\n",
        "        if self.col_item not in pred_columns:\n",
        "            raise ValueError(\n",
        "                \"Schema of rating_pred not valid. Missing Item Col\"\n",
        "            )  # pragma : No Cover\n",
        "        if self.col_prediction not in pred_columns:\n",
        "            raise ValueError(\"Schema of rating_pred not valid. Missing Prediction Col\")\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "        relevant_func = {\n",
        "            \"top_k\": _get_top_k_items,\n",
        "            \"by_time_stamp\": _get_relevant_items_by_timestamp,\n",
        "            \"by_threshold\": _get_relevant_items_by_threshold,\n",
        "        }\n",
        "\n",
        "        if relevancy_method not in relevant_func:\n",
        "            raise ValueError(\n",
        "                \"relevancy_method should be one of {}\".format(\n",
        "                    list(relevant_func.keys())\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.rating_pred = (\n",
        "            relevant_func[relevancy_method](\n",
        "                dataframe=self.rating_pred,\n",
        "                col_user=self.col_user,\n",
        "                col_item=self.col_item,\n",
        "                col_rating=self.col_prediction,\n",
        "                threshold=self.threshold,\n",
        "            )\n",
        "            if relevancy_method == \"by_threshold\"\n",
        "            else relevant_func[relevancy_method](\n",
        "                dataframe=self.rating_pred,\n",
        "                col_user=self.col_user,\n",
        "                col_item=self.col_item,\n",
        "                col_rating=self.col_prediction,\n",
        "                k=self.k,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self._metrics = self._calculate_metrics()\n",
        "\n",
        "    def _calculate_metrics(self):\n",
        "        \"\"\"Calculate ranking metrics.\"\"\"\n",
        "        self._items_for_user_pred = self.rating_pred\n",
        "\n",
        "        self._items_for_user_true = (\n",
        "            self.rating_true.groupBy(self.col_user)\n",
        "            .agg(expr(\"collect_list(\" + self.col_item + \") as ground_truth\"))\n",
        "            .select(self.col_user, \"ground_truth\")\n",
        "        )\n",
        "\n",
        "        self._items_for_user_all = self._items_for_user_pred.join(\n",
        "            self._items_for_user_true, on=self.col_user\n",
        "        ).drop(self.col_user)\n",
        "\n",
        "        return RankingMetrics(self._items_for_user_all.rdd)\n",
        "\n",
        "    def precision_at_k(self):\n",
        "        \"\"\"Get precision@k.\n",
        "        .. note::\n",
        "            More details can be found\n",
        "            `here <http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt>`_.\n",
        "        Return:\n",
        "            float: precision at k (min=0, max=1)\n",
        "        \"\"\"\n",
        "        precision = self._metrics.precisionAt(self.k)\n",
        "\n",
        "        return precision\n",
        "\n",
        "    def recall_at_k(self):\n",
        "        \"\"\"Get recall@K.\n",
        "        .. note::\n",
        "            More details can be found\n",
        "            `here <http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.meanAveragePrecision>`_.\n",
        "        Return:\n",
        "            float: recall at k (min=0, max=1).\n",
        "        \"\"\"\n",
        "        recall = self._items_for_user_all.rdd.map(\n",
        "            lambda x: float(len(set(x[0]).intersection(set(x[1])))) / float(len(x[1]))\n",
        "        ).mean()\n",
        "\n",
        "        return recall\n",
        "\n",
        "    def ndcg_at_k(self):\n",
        "        \"\"\"Get Normalized Discounted Cumulative Gain (NDCG)\n",
        "        .. note::\n",
        "            More details can be found\n",
        "            `here <http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.ndcgAt>`_.\n",
        "        Return:\n",
        "            float: nDCG at k (min=0, max=1).\n",
        "        \"\"\"\n",
        "        ndcg = self._metrics.ndcgAt(self.k)\n",
        "\n",
        "        return ndcg\n",
        "\n",
        "    def map_at_k(self):\n",
        "        \"\"\"Get mean average precision at k.\n",
        "        .. note::\n",
        "            More details can be found\n",
        "            `here <http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.meanAveragePrecision>`_.\n",
        "        Return:\n",
        "            float: MAP at k (min=0, max=1).\n",
        "        \"\"\"\n",
        "        maprecision = self._metrics.meanAveragePrecision\n",
        "\n",
        "        return maprecision\n",
        "\n",
        "\n",
        "def _get_top_k_items(\n",
        "    dataframe,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    k=50,\n",
        "):\n",
        "    \"\"\"Get the input customer-item-rating tuple in the format of Spark\n",
        "    DataFrame, output a Spark DataFrame in the dense format of top k items\n",
        "    for each user.\n",
        "    .. note::\n",
        "        if it is implicit rating, just append a column of constants to be ratings.\n",
        "    Args:\n",
        "        dataframe (pyspark.sql.DataFrame): DataFrame of rating data (in the format of\n",
        "        customerID-itemID-rating tuple).\n",
        "        col_user (str): column name for user.\n",
        "        col_item (str): column name for item.\n",
        "        col_rating (str): column name for rating.\n",
        "        col_prediction (str): column name for prediction.\n",
        "        k (int): number of items for each user.\n",
        "    Return:\n",
        "        pyspark.sql.DataFrame: DataFrame of top k items for each user.\n",
        "    \"\"\"\n",
        "    window_spec = Window.partitionBy(col_user).orderBy(col(col_rating).desc())\n",
        "\n",
        "    # this does not work for rating of the same value.\n",
        "    items_for_user = (\n",
        "        dataframe.select(\n",
        "            col_user, col_item, col_rating, row_number().over(window_spec).alias(\"rank\")\n",
        "        )\n",
        "        .where(col(\"rank\") <= k)\n",
        "        .groupby(col_user)\n",
        "        .agg(F.collect_list(col_item).alias(col_prediction))\n",
        "    )\n",
        "\n",
        "    return items_for_user\n",
        "\n",
        "\n",
        "def _get_relevant_items_by_threshold(\n",
        "    dataframe,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    threshold=50,\n",
        "):\n",
        "    \"\"\"Get relevant items for each customer in the input rating data.\n",
        "    Relevant items are defined as those having ratings above certain threshold.\n",
        "    The threshold is defined as a statistical measure of the ratings for a\n",
        "    user, e.g., median.\n",
        "    Args:\n",
        "        dataframe: Spark DataFrame of customerID-itemID-rating tuples.\n",
        "        col_user (str): column name for user.\n",
        "        col_item (str): column name for item.\n",
        "        col_rating (str): column name for rating.\n",
        "        col_prediction (str): column name for prediction.\n",
        "        threshold (float): threshold for determining the relevant recommended items.\n",
        "            This is used for the case that predicted ratings follow a known\n",
        "            distribution.\n",
        "    Return:\n",
        "        pyspark.sql.DataFrame: DataFrame of customerID-itemID-rating tuples with only relevant\n",
        "        items.\n",
        "    \"\"\"\n",
        "    items_for_user = (\n",
        "        dataframe.orderBy(col_rating, ascending=False)\n",
        "        .where(col_rating + \" >= \" + str(threshold))\n",
        "        .select(col_user, col_item, col_rating)\n",
        "        .withColumn(\n",
        "            col_prediction, F.collect_list(col_item).over(Window.partitionBy(col_user))\n",
        "        )\n",
        "        .select(col_user, col_prediction)\n",
        "        .dropDuplicates()\n",
        "    )\n",
        "\n",
        "    return items_for_user\n",
        "\n",
        "\n",
        "def _get_relevant_items_by_timestamp(\n",
        "    dataframe,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_timestamp=DEFAULT_TIMESTAMP_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    k=50,\n",
        "):\n",
        "    \"\"\"Get relevant items for each customer defined by timestamp.\n",
        "    Relevant items are defined as k items that appear mostly recently\n",
        "    according to timestamps.\n",
        "    Args:\n",
        "        dataframe (pyspark.sql.DataFrame): A Spark DataFrame of customerID-itemID-rating-timeStamp\n",
        "            tuples.\n",
        "        col_user (str): column name for user.\n",
        "        col_item (str): column name for item.\n",
        "        col_rating (str): column name for rating.\n",
        "        col_timestamp (str): column name for timestamp.\n",
        "        col_prediction (str): column name for prediction.\n",
        "        k: number of relevent items to be filtered by the function.\n",
        "    Return:\n",
        "        pyspark.sql.DataFrame: DataFrame of customerID-itemID-rating tuples with only relevant items.\n",
        "    \"\"\"\n",
        "    window_spec = Window.partitionBy(col_user).orderBy(col(col_timestamp).desc())\n",
        "\n",
        "    items_for_user = (\n",
        "        dataframe.select(\n",
        "            col_user, col_item, col_rating, row_number().over(window_spec).alias(\"rank\")\n",
        "        )\n",
        "        .where(col(\"rank\") <= k)\n",
        "        .withColumn(\n",
        "            col_prediction, F.collect_list(col_item).over(Window.partitionBy(col_user))\n",
        "        )\n",
        "        .select(col_user, col_prediction)\n",
        "        .dropDuplicates([col_user, col_prediction])\n",
        "    )\n",
        "\n",
        "    return items_for_user\n",
        "\n",
        "\n",
        "class SparkDiversityEvaluation:\n",
        "    \"\"\"Spark Evaluator for diversity, coverage, novelty, serendipity\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_df,\n",
        "        reco_df,\n",
        "        item_feature_df=None,\n",
        "        item_sim_measure=DEFAULT_ITEM_SIM_MEASURE,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        col_relevance=None,\n",
        "    ):\n",
        "\n",
        "        self.train_df = train_df.select(col_user, col_item)\n",
        "        self.col_user = col_user\n",
        "        self.col_item = col_item\n",
        "        self.sim_col = DEFAULT_SIMILARITY_COL\n",
        "        self.df_cosine_similarity = None\n",
        "        self.df_user_item_serendipity = None\n",
        "        self.df_user_serendipity = None\n",
        "        self.avg_serendipity = None\n",
        "        self.df_item_novelty = None\n",
        "        self.avg_novelty = None\n",
        "        self.df_intralist_similarity = None\n",
        "        self.df_user_diversity = None\n",
        "        self.avg_diversity = None\n",
        "        self.item_feature_df = item_feature_df\n",
        "        self.item_sim_measure = item_sim_measure\n",
        "\n",
        "        if col_relevance is None:\n",
        "            self.col_relevance = DEFAULT_RELEVANCE_COL\n",
        "            # relevance term, default is 1 (relevant) for all\n",
        "            self.reco_df = reco_df.select(\n",
        "                col_user, col_item, F.lit(1.0).alias(self.col_relevance)\n",
        "            )\n",
        "        else:\n",
        "            self.col_relevance = col_relevance\n",
        "            self.reco_df = reco_df.select(\n",
        "                col_user, col_item, F.col(self.col_relevance).cast(DoubleType())\n",
        "            )\n",
        "\n",
        "        if self.item_sim_measure == \"item_feature_vector\":\n",
        "            self.col_item_features = DEFAULT_ITEM_FEATURES_COL\n",
        "            required_schema = StructType(\n",
        "                (\n",
        "                    StructField(self.col_item, IntegerType()),\n",
        "                    StructField(self.col_item_features, VectorUDT()),\n",
        "                )\n",
        "            )\n",
        "            if self.item_feature_df is not None:\n",
        "\n",
        "                if str(required_schema) != str(item_feature_df.schema):\n",
        "                    raise Exception(\n",
        "                        \"Incorrect schema! item_feature_df should have schema:\"\n",
        "                        + str(required_schema)\n",
        "                    )\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"item_feature_df not specified! item_feature_df must be provided \"\n",
        "                    \"if choosing to use item_feature_vector to calculate item similarity. \"\n",
        "                    \"item_feature_df should have schema:\" + str(required_schema)\n",
        "                )\n",
        "\n",
        "        # check if reco_df contains any user_item pairs that are already shown in train_df\n",
        "        count_intersection = (\n",
        "            self.train_df.select(self.col_user, self.col_item)\n",
        "            .intersect(self.reco_df.select(self.col_user, self.col_item))\n",
        "            .count()\n",
        "        )\n",
        "\n",
        "        if count_intersection != 0:\n",
        "            raise Exception(\n",
        "                \"reco_df should not contain any user_item pairs that are already shown in train_df\"\n",
        "            )\n",
        "\n",
        "    def _get_pairwise_items(self, df):\n",
        "        return (\n",
        "            df.select(self.col_user, F.col(self.col_item).alias(\"i1\"))\n",
        "            .join(\n",
        "                df.select(\n",
        "                    F.col(self.col_user).alias(\"_user\"),\n",
        "                    F.col(self.col_item).alias(\"i2\"),\n",
        "                ),\n",
        "                (F.col(self.col_user) == F.col(\"_user\")) & (F.col(\"i1\") <= F.col(\"i2\")),\n",
        "            )\n",
        "            .select(self.col_user, \"i1\", \"i2\")\n",
        "        )\n",
        "\n",
        "    def _get_cosine_similarity(self, n_partitions=200):\n",
        "\n",
        "        if self.item_sim_measure == \"item_cooccurrence_count\":\n",
        "            # calculate item-item similarity based on item co-occurrence count\n",
        "            self._get_cooccurrence_similarity(n_partitions)\n",
        "        elif self.item_sim_measure == \"item_feature_vector\":\n",
        "            # calculate item-item similarity based on item feature vectors\n",
        "            self._get_item_feature_similarity(n_partitions)\n",
        "        else:\n",
        "            raise Exception(\n",
        "                \"item_sim_measure not recognized! The available options include 'item_cooccurrence_count' and 'item_feature_vector'.\"\n",
        "            )\n",
        "        return self.df_cosine_similarity\n",
        "\n",
        "    def _get_cooccurrence_similarity(self, n_partitions):\n",
        "        if self.df_cosine_similarity is None:\n",
        "            pairs = self._get_pairwise_items(df=self.train_df)\n",
        "            item_count = self.train_df.groupBy(self.col_item).count()\n",
        "\n",
        "            self.df_cosine_similarity = (\n",
        "                pairs.groupBy(\"i1\", \"i2\")\n",
        "                .count()\n",
        "                .join(\n",
        "                    item_count.select(\n",
        "                        F.col(self.col_item).alias(\"i1\"),\n",
        "                        F.pow(F.col(\"count\"), 0.5).alias(\"i1_sqrt_count\"),\n",
        "                    ),\n",
        "                    on=\"i1\",\n",
        "                )\n",
        "                .join(\n",
        "                    item_count.select(\n",
        "                        F.col(self.col_item).alias(\"i2\"),\n",
        "                        F.pow(F.col(\"count\"), 0.5).alias(\"i2_sqrt_count\"),\n",
        "                    ),\n",
        "                    on=\"i2\",\n",
        "                )\n",
        "                .select(\n",
        "                    \"i1\",\n",
        "                    \"i2\",\n",
        "                    (\n",
        "                        F.col(\"count\")\n",
        "                        / (F.col(\"i1_sqrt_count\") * F.col(\"i2_sqrt_count\"))\n",
        "                    ).alias(self.sim_col),\n",
        "                )\n",
        "                .repartition(n_partitions, \"i1\", \"i2\")\n",
        "            )\n",
        "        return self.df_cosine_similarity\n",
        "\n",
        "    @staticmethod\n",
        "    @udf(returnType=DoubleType())\n",
        "    def sim_cos(v1, v2):\n",
        "        p = 2\n",
        "        return float(v1.dot(v2)) / float(v1.norm(p) * v2.norm(p))\n",
        "\n",
        "    def _get_item_feature_similarity(self, n_partitions):\n",
        "      \n",
        "        if self.df_cosine_similarity is None:\n",
        "            self.df_cosine_similarity = (\n",
        "                self.item_feature_df.select(\n",
        "                    F.col(self.col_item).alias(\"i1\"),\n",
        "                    F.col(self.col_item_features).alias(\"f1\"),\n",
        "                )\n",
        "                .join(\n",
        "                    self.item_feature_df.select(\n",
        "                        F.col(self.col_item).alias(\"i2\"),\n",
        "                        F.col(self.col_item_features).alias(\"f2\"),\n",
        "                    ),\n",
        "                    (F.col(\"i1\") <= F.col(\"i2\")),\n",
        "                )\n",
        "                .select(\"i1\", \"i2\", self.sim_cos(\"f1\", \"f2\").alias(\"sim\"))\n",
        "                .sort(\"i1\", \"i2\")\n",
        "                .repartition(n_partitions, \"i1\", \"i2\")\n",
        "            )\n",
        "        return self.df_cosine_similarity\n",
        "\n",
        "    # Diversity metrics\n",
        "    def _get_intralist_similarity(self, df):\n",
        "        if self.df_intralist_similarity is None:\n",
        "            pairs = self._get_pairwise_items(df=df)\n",
        "            similarity_df = self._get_cosine_similarity()\n",
        "            # Fillna(0) is needed in the cases where similarity_df does not have an entry for a pair of items.\n",
        "            # e.g. i1 and i2 have never occurred together.\n",
        "            self.df_intralist_similarity = (\n",
        "                pairs.join(similarity_df, on=[\"i1\", \"i2\"], how=\"left\")\n",
        "                .fillna(0)\n",
        "                .filter(F.col(\"i1\") != F.col(\"i2\"))\n",
        "                .groupBy(self.col_user)\n",
        "                .agg(F.mean(self.sim_col).alias(\"avg_il_sim\"))\n",
        "                .select(self.col_user, \"avg_il_sim\")\n",
        "            )\n",
        "        return self.df_intralist_similarity\n",
        "\n",
        "    def user_diversity(self):\n",
        "        if self.df_user_diversity is None:\n",
        "            self.df_intralist_similarity = self._get_intralist_similarity(self.reco_df)\n",
        "            self.df_user_diversity = (\n",
        "                self.df_intralist_similarity.withColumn(\n",
        "                    \"user_diversity\", 1 - F.col(\"avg_il_sim\")\n",
        "                )\n",
        "                .select(self.col_user, \"user_diversity\")\n",
        "                .orderBy(self.col_user)\n",
        "            )\n",
        "        return self.df_user_diversity\n",
        "\n",
        "    def diversity(self):\n",
        "        if self.avg_diversity is None:\n",
        "            self.df_user_diversity = self.user_diversity()\n",
        "            self.avg_diversity = self.df_user_diversity.agg(\n",
        "                {\"user_diversity\": \"mean\"}\n",
        "            ).first()[0]\n",
        "        return self.avg_diversity\n",
        "\n",
        "    # Novelty metrics\n",
        "    def historical_item_novelty(self):\n",
        "        if self.df_item_novelty is None:\n",
        "            n_records = self.train_df.count()\n",
        "            self.df_item_novelty = (\n",
        "                self.train_df.groupBy(self.col_item)\n",
        "                .count()\n",
        "                .withColumn(\"item_novelty\", -F.log2(F.col(\"count\") / n_records))\n",
        "                .select(self.col_item, \"item_novelty\")\n",
        "                .orderBy(self.col_item)\n",
        "            )\n",
        "        return self.df_item_novelty\n",
        "\n",
        "    def novelty(self):\n",
        "        if self.avg_novelty is None:\n",
        "            self.df_item_novelty = self.historical_item_novelty()\n",
        "            n_recommendations = self.reco_df.count()\n",
        "            self.avg_novelty = (\n",
        "                self.reco_df.groupBy(self.col_item)\n",
        "                .count()\n",
        "                .join(self.df_item_novelty, self.col_item)\n",
        "                .selectExpr(\"sum(count * item_novelty)\")\n",
        "                .first()[0]\n",
        "                / n_recommendations\n",
        "            )\n",
        "        return self.avg_novelty\n",
        "\n",
        "    # Serendipity metrics\n",
        "    def user_item_serendipity(self):\n",
        "        # for every col_user, col_item in reco_df, join all interacted items from train_df.\n",
        "        # These interacted items are repeated for each item in reco_df for a specific user.\n",
        "        if self.df_user_item_serendipity is None:\n",
        "            self.df_cosine_similarity = self._get_cosine_similarity()\n",
        "            self.df_user_item_serendipity = (\n",
        "                self.reco_df.select(\n",
        "                    self.col_user,\n",
        "                    self.col_item,\n",
        "                    F.col(self.col_item).alias(\n",
        "                        \"reco_item_tmp\"\n",
        "                    ),  # duplicate col_item to keep\n",
        "                )\n",
        "                .join(\n",
        "                    self.train_df.select(\n",
        "                        self.col_user, F.col(self.col_item).alias(\"train_item_tmp\")\n",
        "                    ),\n",
        "                    on=[self.col_user],\n",
        "                )\n",
        "                .select(\n",
        "                    self.col_user,\n",
        "                    self.col_item,\n",
        "                    F.least(F.col(\"reco_item_tmp\"), F.col(\"train_item_tmp\")).alias(\n",
        "                        \"i1\"\n",
        "                    ),\n",
        "                    F.greatest(F.col(\"reco_item_tmp\"), F.col(\"train_item_tmp\")).alias(\n",
        "                        \"i2\"\n",
        "                    ),\n",
        "                )\n",
        "                .join(self.df_cosine_similarity, on=[\"i1\", \"i2\"], how=\"left\")\n",
        "                .fillna(0)\n",
        "                .groupBy(self.col_user, self.col_item)\n",
        "                .agg(F.mean(self.sim_col).alias(\"avg_item2interactedHistory_sim\"))\n",
        "                .join(self.reco_df, on=[self.col_user, self.col_item])\n",
        "                .withColumn(\n",
        "                    \"user_item_serendipity\",\n",
        "                    (1 - F.col(\"avg_item2interactedHistory_sim\"))\n",
        "                    * F.col(self.col_relevance),\n",
        "                )\n",
        "                .select(self.col_user, self.col_item, \"user_item_serendipity\")\n",
        "                .orderBy(self.col_user, self.col_item)\n",
        "            )\n",
        "        return self.df_user_item_serendipity\n",
        "\n",
        "    def user_serendipity(self):\n",
        "        if self.df_user_serendipity is None:\n",
        "            self.df_user_item_serendipity = self.user_item_serendipity()\n",
        "            self.df_user_serendipity = (\n",
        "                self.df_user_item_serendipity.groupBy(self.col_user)\n",
        "                .agg(F.mean(\"user_item_serendipity\").alias(\"user_serendipity\"))\n",
        "                .orderBy(self.col_user)\n",
        "            )\n",
        "        return self.df_user_serendipity\n",
        "\n",
        "    def serendipity(self):\n",
        "        if self.avg_serendipity is None:\n",
        "            self.df_user_serendipity = self.user_serendipity()\n",
        "            self.avg_serendipity = self.df_user_serendipity.agg(\n",
        "                {\"user_serendipity\": \"mean\"}\n",
        "            ).first()[0]\n",
        "        return self.avg_serendipity\n",
        "\n",
        "    # Coverage metrics\n",
        "    def catalog_coverage(self):\n",
        "        # distinct item count in reco_df\n",
        "        count_distinct_item_reco = self.reco_df.select(self.col_item).distinct().count()\n",
        "        # distinct item count in train_df\n",
        "        count_distinct_item_train = (\n",
        "            self.train_df.select(self.col_item).distinct().count()\n",
        "        )\n",
        "\n",
        "        # catalog coverage\n",
        "        c_coverage = count_distinct_item_reco / count_distinct_item_train\n",
        "        return c_coverage\n",
        "\n",
        "    def distributional_coverage(self):\n",
        "        # In reco_df, how  many times each col_item is being recommended\n",
        "        df_itemcnt_reco = self.reco_df.groupBy(self.col_item).count()\n",
        "\n",
        "        # the number of total recommendations\n",
        "        count_row_reco = self.reco_df.count()\n",
        "        df_entropy = df_itemcnt_reco.withColumn(\n",
        "            \"p(i)\", F.col(\"count\") / count_row_reco\n",
        "        ).withColumn(\"entropy(i)\", F.col(\"p(i)\") * F.log2(F.col(\"p(i)\")))\n",
        "        # distributional coverage\n",
        "        d_coverage = -df_entropy.agg(F.sum(\"entropy(i)\")).collect()[0][0]\n",
        "\n",
        "        return d_coverage"
      ],
      "metadata": {
        "id": "oYkOp5MLlhjR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=COL_USER, col_item=COL_ITEM, \n",
        "                                    col_rating=COL_RATING, col_prediction=\"prediction\", \n",
        "                                    relevancy_method=\"top_k\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6XvGfy5lTxr",
        "outputId": "e71b0447-e5c2-41c5-a827-148afda6a847"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model:\\tALS\",\n",
        "      \"Top K:\\t%d\" % rank_eval.k,\n",
        "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
        "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
        "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
        "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44MeH2A7lT81",
        "outputId": "377be616-4769-406a-a4c5-884ac7b22735"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:\tALS\n",
            "Top K:\t50\n",
            "MAP:\t0.002609\n",
            "NDCG:\t0.024879\n",
            "Precision@K:\t0.017600\n",
            "Recall@K:\t0.034297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BSjOJIDA5ENP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "v2.2_ML20M_sequence.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "17fHxkHoAQq6"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}