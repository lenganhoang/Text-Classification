{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lenganhoang/Text-Classification/blob/master/DNNs_MovieLens!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "# gives a single float value\n",
        "print(psutil.cpu_percent())\n",
        "# gives an object with many fields\n",
        "print(psutil.virtual_memory())\n",
        "# you can convert that object to a dictionary print(dict(psutil.virtual_memory()._asdict()))\n",
        "# you can have the percentage of used RAM\n",
        "print(psutil.virtual_memory().percent)\n",
        "# you can calculate percentage of available memory\n",
        "print(psutil.virtual_memory().available * 100 / psutil.virtual_memory().total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHhpfyaKh1Nr",
        "outputId": "1f692edc-1aad-4940-f90b-560247cd003c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.4\n",
            "svmem(total=13617737728, available=12334903296, percent=9.4, used=1089286144, free=10580566016, active=1447059456, inactive=1378476032, buffers=110194688, cached=1837690880, shared=1228800, slab=130179072)\n",
            "9.4\n",
            "90.57968028446965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_slim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVR4n9ffjOAJ",
        "outputId": "9f115c8f-104c-4f73-e3df-e1fec5af5259"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf_slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import gc\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "WjpELanliFNx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "fpath = \"/content/gdrive/MyDrive/ml_20M/ml-20m/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZtHi9wqiMk0",
        "outputId": "be6124d3-822a-4427-9b1d-8a3bb2641c16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data\n",
        "ratings = pd.read_csv(fpath+'/ratings.csv')\n",
        "\n",
        "ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "cTyb76XFiPFs",
        "outputId": "eb1466b5-edb4-4807-fe0c-cef5d8be3882"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          userId  movieId  rating   timestamp\n",
              "0              1        2     3.5  1112486027\n",
              "1              1       29     3.5  1112484676\n",
              "2              1       32     3.5  1112484819\n",
              "3              1       47     3.5  1112484727\n",
              "4              1       50     3.5  1112484580\n",
              "...          ...      ...     ...         ...\n",
              "20000258  138493    68954     4.5  1258126920\n",
              "20000259  138493    69526     4.5  1259865108\n",
              "20000260  138493    69644     3.0  1260209457\n",
              "20000261  138493    70286     5.0  1258126944\n",
              "20000262  138493    71619     2.5  1255811136\n",
              "\n",
              "[20000263 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-76abc9bb-3555-4482-9da2-d450dc8cb321\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112486027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000258</th>\n",
              "      <td>138493</td>\n",
              "      <td>68954</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1258126920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000259</th>\n",
              "      <td>138493</td>\n",
              "      <td>69526</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1259865108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000260</th>\n",
              "      <td>138493</td>\n",
              "      <td>69644</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1260209457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000261</th>\n",
              "      <td>138493</td>\n",
              "      <td>70286</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1258126944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000262</th>\n",
              "      <td>138493</td>\n",
              "      <td>71619</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1255811136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000263 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76abc9bb-3555-4482-9da2-d450dc8cb321')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-76abc9bb-3555-4482-9da2-d450dc8cb321 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-76abc9bb-3555-4482-9da2-d450dc8cb321');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_USER_COL = \"userId\"\n",
        "DEFAULT_ITEM_COL = \"movieId\"\n",
        "DEFAULT_RATING_COL = \"rating\"\n",
        "DEFAULT_LABEL_COL = \"label\"\n",
        "DEFAULT_TITLE_COL = \"title\"\n",
        "DEFAULT_GENRE_COL = \"genre\"\n",
        "DEFAULT_RELEVANCE_COL = \"relevance\"\n",
        "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
        "DEFAULT_PREDICTION_COL = \"prediction\"\n",
        "DEFAULT_SIMILARITY_COL = \"sim\"\n",
        "DEFAULT_ITEM_FEATURES_COL = \"features\"\n",
        "DEFAULT_ITEM_SIM_MEASURE = \"item_cooccurrence_count\""
      ],
      "metadata": {
        "id": "I0TQIioSiW7g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import OrderedDict\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class EmptyFileException(Exception):\n",
        "    \"\"\"Exception raised if file is empty\"\"\"\n",
        "\n",
        "\n",
        "class MissingFieldsException(Exception):\n",
        "    \"\"\"Exception raised if file is missing expected fields\"\"\"\n",
        "\n",
        "\n",
        "class FileNotSortedException(Exception):\n",
        "    \"\"\"Exception raised if file is not sorted correctly\"\"\"\n",
        "\n",
        "\n",
        "class MissingUserException(Exception):\n",
        "    \"\"\"Exception raised if user is not in file\"\"\"\n",
        "\n",
        "\n",
        "class DataFile:\n",
        "    \"\"\"\n",
        "    DataFile class for NCF. Iterator to read data from a csv file.\n",
        "    Data must be sorted by user. Includes utilities for loading user data from\n",
        "    file, formatting it and returning a Pandas dataframe.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, filename, col_user, col_item, col_rating, col_test_batch=None, binary=True\n",
        "    ):\n",
        "        \"\"\"Constructor\n",
        "        Args:\n",
        "            filename (str): Path to file to be processed.\n",
        "            col_user (str): User column name.\n",
        "            col_item (str): Item column name.\n",
        "            col_rating (str): Rating column name.\n",
        "            col_test_batch (str): Test batch column name.\n",
        "            binary (bool): If true, set rating > 0 to rating = 1.\n",
        "        \"\"\"\n",
        "        self.filename = filename\n",
        "        self.col_user = col_user\n",
        "        self.col_item = col_item\n",
        "        self.col_rating = col_rating\n",
        "        self.col_test_batch = col_test_batch\n",
        "        self.expected_fields = [self.col_user, self.col_item, self.col_rating]\n",
        "        if self.col_test_batch is not None:\n",
        "            self.expected_fields.append(self.col_test_batch)\n",
        "        self.binary = binary\n",
        "        self._init_data()\n",
        "        self.id2user = {self.user2id[k]: k for k in self.user2id}\n",
        "        self.id2item = {self.item2id[k]: k for k in self.item2id}\n",
        "\n",
        "    @property\n",
        "    def users(self):\n",
        "        return self.user2id.keys()\n",
        "\n",
        "    @property\n",
        "    def items(self):\n",
        "        return self.item2id.keys()\n",
        "\n",
        "    @property\n",
        "    def end_of_file(self):\n",
        "        return (self.line_num > 0) and self.next_row is None\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __enter__(self, *args):\n",
        "        self.file = open(self.filename, \"r\", encoding=\"UTF8\")\n",
        "        self.reader = csv.DictReader(self.file)\n",
        "        self._check_for_missing_fields(self.expected_fields)\n",
        "        self.line_num = 0\n",
        "        self.row, self.next_row = None, None\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.file.close()\n",
        "        self.reader = None\n",
        "        self.line_num = 0\n",
        "        self.row, self.next_row = None, None\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.next_row:\n",
        "            self.row = self.next_row\n",
        "        elif self.line_num == 0:\n",
        "            self.row = self._extract_row_data(next(self.reader, None))\n",
        "            if self.row is None:\n",
        "                raise EmptyFileException(\"{} is empty.\".format(self.filename))\n",
        "        else:\n",
        "            raise StopIteration  # end of file\n",
        "        self.next_row = self._extract_row_data(next(self.reader, None))\n",
        "        self.line_num += 1\n",
        "\n",
        "        return self.row\n",
        "\n",
        "    def _check_for_missing_fields(self, fields_to_check):\n",
        "        missing_fields = set(fields_to_check).difference(set(self.reader.fieldnames))\n",
        "        if len(missing_fields):\n",
        "            raise MissingFieldsException(\n",
        "                \"Columns {} not in header of file {}\".format(\n",
        "                    missing_fields, self.filename\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def _extract_row_data(self, row):\n",
        "        if row is None:\n",
        "            return row\n",
        "        user = int(row[self.col_user])\n",
        "        item = int(row[self.col_item])\n",
        "        rating = float(row[self.col_rating])\n",
        "        if self.binary:\n",
        "            rating = float(rating > 0)\n",
        "        test_batch = None\n",
        "        if self.col_test_batch:\n",
        "            test_batch = int(row[self.col_test_batch])\n",
        "        return {\n",
        "            self.col_user: user,\n",
        "            self.col_item: item,\n",
        "            self.col_rating: rating,\n",
        "            self.col_test_batch: test_batch,\n",
        "        }\n",
        "\n",
        "    def _init_data(self):\n",
        "        # Compile lists of unique users and items, assign IDs to users and items,\n",
        "        # and ensure file is sorted by user (and batch index if test set)\n",
        "        logger.info(\"Indexing {} ...\".format(self.filename))\n",
        "        with self:\n",
        "            user_items = []\n",
        "            self.item2id, self.user2id = OrderedDict(), OrderedDict()\n",
        "            batch_index = 0\n",
        "            for _ in self:\n",
        "                item = self.row[self.col_item]\n",
        "                user = self.row[self.col_user]\n",
        "                test_batch = self.row[self.col_test_batch]\n",
        "                if not self.end_of_file:\n",
        "                    next_user = self.next_row[self.col_user]\n",
        "                    next_test_batch = self.next_row[self.col_test_batch]\n",
        "                if item not in self.items:\n",
        "                    self.item2id[item] = len(self.item2id)\n",
        "                user_items.append(item)\n",
        "\n",
        "                if (next_user != user) or self.next_row is None:\n",
        "                    if not self.end_of_file:\n",
        "                        if next_user in self.users:\n",
        "                            raise FileNotSortedException(\n",
        "                                \"File {} is not sorted by user\".format(self.filename)\n",
        "                            )\n",
        "                    self.user2id[user] = len(self.user2id)\n",
        "                if self.col_test_batch:\n",
        "                    if (next_test_batch != test_batch) or self.next_row is None:\n",
        "                        if not self.end_of_file:\n",
        "                            if next_test_batch < batch_index:\n",
        "                                raise FileNotSortedException(\n",
        "                                    \"File {} is not sorted by {}\".format(\n",
        "                                        self.filename, self.col_test_batch\n",
        "                                    )\n",
        "                                )\n",
        "                        batch_index += 1\n",
        "            self.batch_indices_range = range(0, batch_index)\n",
        "            self.data_len = self.line_num\n",
        "\n",
        "    def load_data(self, key, by_user=True):\n",
        "        \"\"\"Load data for a specified user or test batch\n",
        "        Args:\n",
        "            key (int): user or test batch index\n",
        "            by_user (bool): load data by usr if True, else by test batch\n",
        "        Returns:\n",
        "            pandas.DataFrame\n",
        "        \"\"\"\n",
        "        records = []\n",
        "        key_col = self.col_user if by_user else self.col_test_batch\n",
        "\n",
        "        # fast forward in file to user/test batch\n",
        "        while (self.line_num == 0) or (self.row[key_col] != key):\n",
        "            if self.end_of_file:\n",
        "                raise MissingUserException(\"User {} not in file {}\".format(key, self.filename))\n",
        "            next(self)\n",
        "        # collect user/test batch data\n",
        "        while self.row[key_col] == key:\n",
        "            row = self.row\n",
        "            if self.col_test_batch in row:\n",
        "                del row[self.col_test_batch]\n",
        "            records.append(row)\n",
        "            if not self.end_of_file:\n",
        "                next(self)\n",
        "            else:\n",
        "                break\n",
        "        return pd.DataFrame.from_records(records)\n",
        "\n",
        "\n",
        "class NegativeSampler:\n",
        "    \"\"\"NegativeSampler class for NCF. Samples a subset of negative items from a given population of items.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        user,\n",
        "        n_samples,\n",
        "        user_positive_item_pool,\n",
        "        item_pool,\n",
        "        sample_with_replacement,\n",
        "        print_warnings=True,\n",
        "        training=True,\n",
        "    ):\n",
        "        \"\"\"Constructor\n",
        "        Args:\n",
        "            user (str or int): User to be sampled for.\n",
        "            n_samples (int): Number of required samples.\n",
        "            user_positive_item_pool (set): Set of items with which user has previously interacted.\n",
        "            item_pool (set): Set of all items in population.\n",
        "            sample_with_replacement (bool): If true, sample negative examples with replacement,\n",
        "                otherwise without replacement.\n",
        "            print_warnings (bool): If true, prints warnings if sampling without replacement and\n",
        "                there are not enough items to sample from to satisfy n_neg or n_neg_test.\n",
        "            training (bool): Set to true if sampling for the training set or false if for the test set.\n",
        "        \"\"\"\n",
        "        self.user = user\n",
        "        self.n_samples = n_samples\n",
        "        self.user_positive_item_pool = user_positive_item_pool\n",
        "        self.item_pool = item_pool\n",
        "        self.sample_with_replacement = sample_with_replacement\n",
        "        self.print_warnings = print_warnings\n",
        "        self.training = training\n",
        "\n",
        "        self.user_negative_item_pool = self._get_user_negatives_pool()\n",
        "        self.population_size = len(self.user_negative_item_pool)\n",
        "        self._sample = (\n",
        "            self._sample_negatives_with_replacement\n",
        "            if self.sample_with_replacement\n",
        "            else self._sample_negatives_without_replacement\n",
        "        )\n",
        "        if not self.sample_with_replacement:\n",
        "            self._check_sample_size()\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Method for sampling uniformly from a population of negative items\n",
        "        Returns: list\n",
        "        \"\"\"\n",
        "        return self._sample()\n",
        "\n",
        "    def _get_user_negatives_pool(self):\n",
        "        # get list of items user has not interacted with\n",
        "        return list(set(self.item_pool) - self.user_positive_item_pool)\n",
        "\n",
        "    def _sample_negatives_with_replacement(self):\n",
        "        return random.choices(self.user_negative_item_pool, k=self.n_samples)\n",
        "\n",
        "    def _sample_negatives_without_replacement(self):\n",
        "        return random.sample(self.user_negative_item_pool, k=self.n_samples)\n",
        "\n",
        "    def _check_sample_size(self):\n",
        "        # if sampling without replacement, check sample population is sufficient and reduce\n",
        "        # n_samples if not.\n",
        "        n_neg_var = \"n_neg\" if self.training else \"n_neg_test\"\n",
        "        dataset_name = \"training\" if self.training else \"test\"\n",
        "\n",
        "        k = min(self.n_samples, self.population_size)\n",
        "        if k < self.n_samples and self.print_warnings:\n",
        "            warning_string = (\n",
        "                \"The population of negative items to sample from is too small for user {}. \"\n",
        "                \"Samples needed = {}, negative items = {}. \"\n",
        "                \"Reducing samples to {} for this user.\"\n",
        "                \"If an equal number of negative samples for each user is required in the {} set, sample with replacement or reduce {}. \"\n",
        "                \"This warning can be turned off by setting print_warnings=False\".format(\n",
        "                    self.user,\n",
        "                    self.n_samples,\n",
        "                    self.population_size,\n",
        "                    self.population_size,\n",
        "                    dataset_name,\n",
        "                    n_neg_var,\n",
        "                )\n",
        "            )\n",
        "            logging.warning(warning_string)\n",
        "        self.n_samples = k\n",
        "\n",
        "\n",
        "class Dataset(object):\n",
        "    \"\"\"Dataset class for NCF\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_file,\n",
        "        test_file=None,\n",
        "        test_file_full=None,\n",
        "        overwrite_test_file_full=False,\n",
        "        n_neg=4,\n",
        "        n_neg_test=100,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        col_rating=DEFAULT_RATING_COL,\n",
        "        binary=True,\n",
        "        seed=None,\n",
        "        sample_with_replacement=False,\n",
        "        print_warnings=False,\n",
        "    ):\n",
        "        \"\"\"Constructor\n",
        "        Args:\n",
        "            train_file (str): Path to training dataset file.\n",
        "            test_file (str): Path to test dataset file for leave-one-out evaluation.\n",
        "            test_file_full (str): Path to full test dataset file including negative samples.\n",
        "            overwrite_test_file_full (bool): If true, recreate and overwrite test_file_full.\n",
        "            n_neg (int): Number of negative samples per positive example for training set.\n",
        "            n_neg_test (int): Number of negative samples per positive example for test set.\n",
        "            col_user (str): User column name.\n",
        "            col_item (str): Item column name.\n",
        "            col_rating (str): Rating column name.\n",
        "            binary (bool): If true, set rating > 0 to rating = 1.\n",
        "            seed (int): Seed.\n",
        "            sample_with_replacement (bool): If true, sample negative examples with replacement,\n",
        "                otherwise without replacement.\n",
        "            print_warnings (bool): If true, prints warnings if sampling without replacement and\n",
        "                there are not enough items to sample from to satisfy n_neg or n_neg_test.\n",
        "        \"\"\"\n",
        "        self.train_file = train_file\n",
        "        self.test_file = test_file\n",
        "        self.test_file_full = test_file_full\n",
        "        self.overwrite_test_file_full = overwrite_test_file_full\n",
        "        self.n_neg = n_neg\n",
        "        self.n_neg_test = n_neg_test\n",
        "        self.col_user = col_user\n",
        "        self.col_item = col_item\n",
        "        self.col_rating = col_rating\n",
        "        self.binary = binary\n",
        "        self.sample_with_replacement = sample_with_replacement\n",
        "        self.print_warnings = print_warnings\n",
        "\n",
        "        self.col_test_batch = \"test_batch\"\n",
        "\n",
        "        self.train_datafile = DataFile(\n",
        "            filename=self.train_file,\n",
        "            col_user=self.col_user,\n",
        "            col_item=self.col_item,\n",
        "            col_rating=self.col_rating,\n",
        "            binary=self.binary,\n",
        "        )\n",
        "\n",
        "        self.n_users = len(self.train_datafile.users)\n",
        "        self.n_items = len(self.train_datafile.items)\n",
        "        self.user2id = self.train_datafile.user2id\n",
        "        self.item2id = self.train_datafile.item2id\n",
        "        self.id2user = self.train_datafile.id2user\n",
        "        self.id2item = self.train_datafile.id2item\n",
        "        self.train_len = self.train_datafile.data_len\n",
        "\n",
        "        if self.test_file is not None:\n",
        "            self.test_datafile = DataFile(\n",
        "                filename=self.test_file,\n",
        "                col_user=self.col_user,\n",
        "                col_item=self.col_item,\n",
        "                col_rating=self.col_rating,\n",
        "                binary=self.binary,\n",
        "            )\n",
        "            if self.test_file_full is None:\n",
        "                self.test_file_full = os.path.splitext(self.test_file)[0] + \"_full.csv\"\n",
        "            if self.overwrite_test_file_full or not os.path.isfile(self.test_file_full):\n",
        "                self._create_test_file()\n",
        "            self.test_full_datafile = DataFile(\n",
        "                filename=self.test_file_full,\n",
        "                col_user=self.col_user,\n",
        "                col_item=self.col_item,\n",
        "                col_rating=self.col_rating,\n",
        "                col_test_batch=self.col_test_batch,\n",
        "                binary=self.binary,\n",
        "            )\n",
        "        # set random seed\n",
        "        random.seed(seed)\n",
        "\n",
        "    def _create_negative_examples_df(self, user, user_negative_samples):\n",
        "        # create dataframe containing negative examples for user assigned zero rating\n",
        "        n_samples = len(user_negative_samples)\n",
        "        return pd.DataFrame(\n",
        "            {\n",
        "                self.col_user: [user] * n_samples,\n",
        "                self.col_item: user_negative_samples,\n",
        "                self.col_rating: [0.0] * n_samples,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def _create_test_file(self):\n",
        "\n",
        "        logger.info(\n",
        "            \"Creating full leave-one-out test file {} ...\".format(self.test_file_full)\n",
        "        )\n",
        "\n",
        "        # create empty csv\n",
        "        pd.DataFrame(\n",
        "            columns=[self.col_user, self.col_item, self.col_rating, self.col_test_batch]\n",
        "        ).to_csv(self.test_file_full, index=False)\n",
        "\n",
        "        batch_idx = 0\n",
        "\n",
        "        with self.train_datafile as train_datafile:\n",
        "            with self.test_datafile as test_datafile:\n",
        "                for user in tqdm(test_datafile.users):\n",
        "                    if user in train_datafile.users:\n",
        "                        user_test_data = test_datafile.load_data(user)\n",
        "                        user_train_data = train_datafile.load_data(user)\n",
        "                        # for leave-one-out evaluation, exclude items seen in both training and test sets\n",
        "                        # when sampling negatives\n",
        "                        user_positive_item_pool = set(\n",
        "                            user_test_data[self.col_item].unique()\n",
        "                        ).union(user_train_data[self.col_item].unique())\n",
        "                        sampler = NegativeSampler(\n",
        "                            user,\n",
        "                            self.n_neg_test,\n",
        "                            user_positive_item_pool,\n",
        "                            self.train_datafile.items,\n",
        "                            self.sample_with_replacement,\n",
        "                            self.print_warnings,\n",
        "                            training=False,\n",
        "                        )\n",
        "\n",
        "                        user_examples_dfs = []\n",
        "                        # sample n_neg_test negatives for each positive example and assign a batch index\n",
        "                        for positive_example in np.array_split(\n",
        "                            user_test_data, user_test_data.shape[0]\n",
        "                        ):\n",
        "                            negative_examples = self._create_negative_examples_df(\n",
        "                                user, sampler.sample()\n",
        "                            )\n",
        "                            examples = pd.concat([positive_example, negative_examples])\n",
        "                            examples[self.col_test_batch] = batch_idx\n",
        "                            user_examples_dfs.append(examples)\n",
        "                            batch_idx += 1\n",
        "                        # append user test data to file\n",
        "                        user_examples = pd.concat(user_examples_dfs)\n",
        "                        user_examples.to_csv(\n",
        "                            self.test_file_full, mode=\"a\", index=False, header=False\n",
        "                        )\n",
        "\n",
        "    def _split_into_batches(self, shuffle_buffer, batch_size):\n",
        "        for i in range(0, len(shuffle_buffer), batch_size):\n",
        "            yield shuffle_buffer[i : i + batch_size]\n",
        "\n",
        "    def _prepare_batch_with_id(self, batch):\n",
        "        return [\n",
        "            [self.user2id[user] for user in batch[self.col_user].values],\n",
        "            [self.item2id[item] for item in batch[self.col_item].values],\n",
        "            batch[self.col_rating].values.tolist(),\n",
        "        ]\n",
        "\n",
        "    def _prepare_batch_without_id(self, batch):\n",
        "        return [\n",
        "            batch[self.col_user].values.tolist(),\n",
        "            batch[self.col_item].values.tolist(),\n",
        "            batch[self.col_rating].values.tolist(),\n",
        "        ]\n",
        "\n",
        "    def _release_shuffle_buffer(\n",
        "        self, shuffle_buffer, batch_size, yield_id, write_to=None\n",
        "    ):\n",
        "        prepare_batch = (\n",
        "            self._prepare_batch_with_id if yield_id else self._prepare_batch_without_id\n",
        "        )\n",
        "        shuffle_buffer_df = pd.concat(shuffle_buffer)\n",
        "        shuffle_buffer_df = shuffle_buffer_df.sample(\n",
        "            shuffle_buffer_df.shape[0]\n",
        "        )  # shuffle the buffer\n",
        "        for batch in self._split_into_batches(shuffle_buffer_df, batch_size):\n",
        "            if batch.shape[0] == batch_size:\n",
        "                if write_to:\n",
        "                    batch.to_csv(write_to, mode=\"a\", header=False, index=False)\n",
        "                yield prepare_batch(batch)\n",
        "            else:\n",
        "                return batch\n",
        "\n",
        "    def train_loader(\n",
        "        self, batch_size, shuffle_size=None, yield_id=False, write_to=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Generator for serving batches of training data. Positive examples are loaded from the\n",
        "        original training file, to which negative samples are added. Data is loaded in memory into a\n",
        "        shuffle buffer up to a maximum of shuffle_size rows, before the data is shuffled and released.\n",
        "        If out-of-memory errors are encountered, try reducing shuffle_size.\n",
        "        Args:\n",
        "            batch_size (int): Number of examples in each batch.\n",
        "            shuffle_size (int): Maximum number of examples in shuffle buffer.\n",
        "            yield_id (bool): If true, return assigned user and item IDs, else return original values.\n",
        "            write_to (str): Path of file to write full dataset (including negative examples).\n",
        "        Returns:\n",
        "            list\n",
        "        \"\"\"\n",
        "\n",
        "        # if shuffle_size not supplied, use (estimated) full data size i.e. complete in-memory shuffle\n",
        "        if shuffle_size is None:\n",
        "            shuffle_size = self.train_len * (self.n_neg + 1)\n",
        "        if write_to:\n",
        "            pd.DataFrame(\n",
        "                columns=[self.col_user, self.col_item, self.col_rating]\n",
        "            ).to_csv(write_to, header=True, index=False)\n",
        "        shuffle_buffer = []\n",
        "\n",
        "        with self.train_datafile as train_datafile:\n",
        "            for user in train_datafile.users:\n",
        "                user_positive_examples = train_datafile.load_data(user)\n",
        "                user_positive_item_pool = set(\n",
        "                    user_positive_examples[self.col_item].unique()\n",
        "                )\n",
        "                n_samples = self.n_neg * user_positive_examples.shape[0]\n",
        "                sampler = NegativeSampler(\n",
        "                    user,\n",
        "                    n_samples,\n",
        "                    user_positive_item_pool,\n",
        "                    self.train_datafile.items,\n",
        "                    self.sample_with_replacement,\n",
        "                    self.print_warnings,\n",
        "                )\n",
        "                user_negative_examples = self._create_negative_examples_df(\n",
        "                    user, sampler.sample()\n",
        "                )\n",
        "                user_examples = pd.concat(\n",
        "                    [user_positive_examples, user_negative_examples]\n",
        "                )\n",
        "                shuffle_buffer.append(user_examples)\n",
        "                shuffle_buffer_len = sum([df.shape[0] for df in shuffle_buffer])\n",
        "                if shuffle_buffer_len >= shuffle_size:\n",
        "                    buffer_remainder = yield from self._release_shuffle_buffer(\n",
        "                        shuffle_buffer, batch_size, yield_id, write_to\n",
        "                    )\n",
        "                    shuffle_buffer = (\n",
        "                        [buffer_remainder] if buffer_remainder is not None else []\n",
        "                    )\n",
        "            # yield remaining buffer\n",
        "            yield from self._release_shuffle_buffer(\n",
        "                shuffle_buffer, batch_size, yield_id, write_to\n",
        "            )\n",
        "\n",
        "    def test_loader(self, yield_id=False):\n",
        "        \"\"\"Generator for serving batches of test data for leave-one-out evaluation. Data is loaded from test_file_full.\n",
        "        Args:\n",
        "            yield_id (bool): If true, return assigned user and item IDs, else return original values.\n",
        "        Returns:\n",
        "            list\n",
        "        \"\"\"\n",
        "        prepare_batch = (\n",
        "            self._prepare_batch_with_id if yield_id else self._prepare_batch_without_id\n",
        "        )\n",
        "\n",
        "        with self.test_full_datafile as test_full_datafile:\n",
        "            for test_batch_idx in test_full_datafile.batch_indices_range:\n",
        "                test_batch_data = test_full_datafile.load_data(\n",
        "                    test_batch_idx, by_user=False\n",
        "                )\n",
        "                yield prepare_batch(test_batch_data)"
      ],
      "metadata": {
        "id": "wZHS88eai_XD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tf_slim as slim\n",
        "from time import time\n",
        "import logging\n",
        "\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "logger = logging.getLogger(__name__)\n",
        "MODEL_CHECKPOINT = \"model.ckpt\"\n",
        "\n",
        "\n",
        "class NCF:\n",
        "    \"\"\"Neural Collaborative Filtering (NCF) implementation\n",
        "    :Citation:\n",
        "        He, Xiangnan, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. \"Neural collaborative filtering.\"\n",
        "        In Proceedings of the 26th International Conference on World Wide Web, pp. 173-182. International World Wide Web\n",
        "        Conferences Steering Committee, 2017. Link: https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_users,\n",
        "        n_items,\n",
        "        model_type=\"NeuMF\",\n",
        "        n_factors=8,\n",
        "        layer_sizes=[16, 8, 4],\n",
        "        n_epochs=50,\n",
        "        batch_size=64,\n",
        "        learning_rate=5e-3,\n",
        "        verbose=1,\n",
        "        seed=None,\n",
        "    ):\n",
        "        \"\"\"Constructor\n",
        "        Args:\n",
        "            n_users (int): Number of users in the dataset.\n",
        "            n_items (int): Number of items in the dataset.\n",
        "            model_type (str): Model type.\n",
        "            n_factors (int): Dimension of latent space.\n",
        "            layer_sizes (list): Number of layers for MLP.\n",
        "            n_epochs (int): Number of epochs for training.\n",
        "            batch_size (int): Batch size.\n",
        "            learning_rate (float): Learning rate.\n",
        "            verbose (int): Whether to show the training output or not.\n",
        "            seed (int): Seed.\n",
        "        \"\"\"\n",
        "\n",
        "        # seed\n",
        "        tf.compat.v1.set_random_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        self.seed = seed\n",
        "\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.model_type = model_type.lower()\n",
        "        self.n_factors = n_factors\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.n_epochs = n_epochs\n",
        "        self.verbose = verbose\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # check model type\n",
        "        model_options = [\"gmf\", \"mlp\", \"neumf\"]\n",
        "        if self.model_type not in model_options:\n",
        "            raise ValueError(\n",
        "                \"Wrong model type, please select one of this list: {}\".format(\n",
        "                    model_options\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # ncf layer input size\n",
        "        self.ncf_layer_size = n_factors + layer_sizes[-1]\n",
        "        # create ncf model\n",
        "        self._create_model()\n",
        "        # set GPU use with demand growth\n",
        "        gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "        # set TF Session\n",
        "        self.sess = tf.compat.v1.Session(\n",
        "            config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
        "        )\n",
        "        # parameters initialization\n",
        "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    def _create_model(\n",
        "        self,\n",
        "    ):\n",
        "        # reset graph\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "\n",
        "        with tf.compat.v1.variable_scope(\"input_data\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "            # input: index of users, items and ground truth\n",
        "            self.user_input = tf.compat.v1.placeholder(tf.int32, shape=[None, 1])\n",
        "            self.item_input = tf.compat.v1.placeholder(tf.int32, shape=[None, 1])\n",
        "            self.labels = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "        with tf.compat.v1.variable_scope(\"embedding\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "            # set embedding table\n",
        "            self.embedding_gmf_P = tf.Variable(\n",
        "                tf.random.truncated_normal(\n",
        "                    shape=[self.n_users, self.n_factors],\n",
        "                    mean=0.0,\n",
        "                    stddev=0.01,\n",
        "                    seed=self.seed,\n",
        "                ),\n",
        "                name=\"embedding_gmf_P\",\n",
        "                dtype=tf.float32,\n",
        "            )\n",
        "\n",
        "            self.embedding_gmf_Q = tf.Variable(\n",
        "                tf.random.truncated_normal(\n",
        "                    shape=[self.n_items, self.n_factors],\n",
        "                    mean=0.0,\n",
        "                    stddev=0.01,\n",
        "                    seed=self.seed,\n",
        "                ),\n",
        "                name=\"embedding_gmf_Q\",\n",
        "                dtype=tf.float32,\n",
        "            )\n",
        "\n",
        "            # set embedding table\n",
        "            self.embedding_mlp_P = tf.Variable(\n",
        "                tf.random.truncated_normal(\n",
        "                    shape=[self.n_users, int(self.layer_sizes[0] / 2)],\n",
        "                    mean=0.0,\n",
        "                    stddev=0.01,\n",
        "                    seed=self.seed,\n",
        "                ),\n",
        "                name=\"embedding_mlp_P\",\n",
        "                dtype=tf.float32,\n",
        "            )\n",
        "\n",
        "            self.embedding_mlp_Q = tf.Variable(\n",
        "                tf.random.truncated_normal(\n",
        "                    shape=[self.n_items, int(self.layer_sizes[0] / 2)],\n",
        "                    mean=0.0,\n",
        "                    stddev=0.01,\n",
        "                    seed=self.seed,\n",
        "                ),\n",
        "                name=\"embedding_mlp_Q\",\n",
        "                dtype=tf.float32,\n",
        "            )\n",
        "\n",
        "        with tf.compat.v1.variable_scope(\"gmf\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "            # get user embedding p and item embedding q\n",
        "            self.gmf_p = tf.reduce_sum(\n",
        "                input_tensor=tf.nn.embedding_lookup(\n",
        "                    params=self.embedding_gmf_P, ids=self.user_input\n",
        "                ),\n",
        "                axis=1,\n",
        "            )\n",
        "            self.gmf_q = tf.reduce_sum(\n",
        "                input_tensor=tf.nn.embedding_lookup(\n",
        "                    params=self.embedding_gmf_Q, ids=self.item_input\n",
        "                ),\n",
        "                axis=1,\n",
        "            )\n",
        "\n",
        "            # get gmf vector\n",
        "            self.gmf_vector = self.gmf_p * self.gmf_q\n",
        "\n",
        "        with tf.compat.v1.variable_scope(\"mlp\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "            # get user embedding p and item embedding q\n",
        "            self.mlp_p = tf.reduce_sum(\n",
        "                input_tensor=tf.nn.embedding_lookup(\n",
        "                    params=self.embedding_mlp_P, ids=self.user_input\n",
        "                ),\n",
        "                axis=1,\n",
        "            )\n",
        "            self.mlp_q = tf.reduce_sum(\n",
        "                input_tensor=tf.nn.embedding_lookup(\n",
        "                    params=self.embedding_mlp_Q, ids=self.item_input\n",
        "                ),\n",
        "                axis=1,\n",
        "            )\n",
        "\n",
        "            # concatenate user and item vector\n",
        "            output = tf.concat([self.mlp_p, self.mlp_q], 1)\n",
        "\n",
        "            # MLP Layers\n",
        "            for layer_size in self.layer_sizes[1:]:\n",
        "                output = slim.layers.fully_connected(\n",
        "                    output,\n",
        "                    num_outputs=layer_size,\n",
        "                    activation_fn=tf.nn.relu,\n",
        "                    weights_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "                        scale=1.0,\n",
        "                        mode=\"fan_avg\",\n",
        "                        distribution=\"uniform\",\n",
        "                        seed=self.seed,\n",
        "                    ),\n",
        "                )\n",
        "            self.mlp_vector = output\n",
        "\n",
        "            # self.output = tf.sigmoid(tf.reduce_sum(self.mlp_vector, axis=1, keepdims=True))\n",
        "\n",
        "        with tf.compat.v1.variable_scope(\"ncf\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "            if self.model_type == \"gmf\":\n",
        "                # GMF only\n",
        "                output = slim.layers.fully_connected(\n",
        "                    self.gmf_vector,\n",
        "                    num_outputs=1,\n",
        "                    activation_fn=None,\n",
        "                    biases_initializer=None,\n",
        "                    weights_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "                        scale=1.0,\n",
        "                        mode=\"fan_avg\",\n",
        "                        distribution=\"uniform\",\n",
        "                        seed=self.seed,\n",
        "                    ),\n",
        "                )\n",
        "                self.output = tf.sigmoid(output)\n",
        "\n",
        "            elif self.model_type == \"mlp\":\n",
        "                # MLP only\n",
        "                output = slim.layers.fully_connected(\n",
        "                    self.mlp_vector,\n",
        "                    num_outputs=1,\n",
        "                    activation_fn=None,\n",
        "                    biases_initializer=None,\n",
        "                    weights_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "                        scale=1.0,\n",
        "                        mode=\"fan_avg\",\n",
        "                        distribution=\"uniform\",\n",
        "                        seed=self.seed,\n",
        "                    ),\n",
        "                )\n",
        "                self.output = tf.sigmoid(output)\n",
        "\n",
        "            elif self.model_type == \"neumf\":\n",
        "                # concatenate GMF and MLP vector\n",
        "                self.ncf_vector = tf.concat([self.gmf_vector, self.mlp_vector], 1)\n",
        "                # get predicted rating score\n",
        "                output = slim.layers.fully_connected(\n",
        "                    self.ncf_vector,\n",
        "                    num_outputs=1,\n",
        "                    activation_fn=None,\n",
        "                    biases_initializer=None,\n",
        "                    weights_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "                        scale=1.0,\n",
        "                        mode=\"fan_avg\",\n",
        "                        distribution=\"uniform\",\n",
        "                        seed=self.seed,\n",
        "                    ),\n",
        "                )\n",
        "                self.output = tf.sigmoid(output)\n",
        "\n",
        "        with tf.compat.v1.variable_scope(\"loss\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "            # set loss function\n",
        "            self.loss = tf.compat.v1.losses.log_loss(self.labels, self.output)\n",
        "\n",
        "        with tf.compat.v1.variable_scope(\"optimizer\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "            # set optimizer\n",
        "            self.optimizer = tf.compat.v1.train.AdamOptimizer(\n",
        "                learning_rate=self.learning_rate\n",
        "            ).minimize(self.loss)\n",
        "\n",
        "    def save(self, dir_name):\n",
        "        \"\"\"Save model parameters in `dir_name`\n",
        "        Args:\n",
        "            dir_name (str): directory name, which should be a folder name instead of file name\n",
        "                we will create a new directory if not existing.\n",
        "        \"\"\"\n",
        "        # save trained model\n",
        "        if not os.path.exists(dir_name):\n",
        "            os.makedirs(dir_name)\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        saver.save(self.sess, os.path.join(dir_name, MODEL_CHECKPOINT))\n",
        "\n",
        "    def load(self, gmf_dir=None, mlp_dir=None, neumf_dir=None, alpha=0.5):\n",
        "        \"\"\"Load model parameters for further use.\n",
        "        GMF model --> load parameters in `gmf_dir`\n",
        "        MLP model --> load parameters in `mlp_dir`\n",
        "        NeuMF model --> load parameters in `neumf_dir` or in `gmf_dir` and `mlp_dir`\n",
        "        Args:\n",
        "            gmf_dir (str): Directory name for GMF model.\n",
        "            mlp_dir (str): Directory name for MLP model.\n",
        "            neumf_dir (str): Directory name for neumf model.\n",
        "            alpha (float): the concatenation hyper-parameter for gmf and mlp output layer.\n",
        "        Returns:\n",
        "            object: Load parameters in this model.\n",
        "        \"\"\"\n",
        "\n",
        "        # load pre-trained model\n",
        "        if self.model_type == \"gmf\" and gmf_dir is not None:\n",
        "            saver = tf.compat.v1.train.Saver()\n",
        "            saver.restore(self.sess, os.path.join(gmf_dir, MODEL_CHECKPOINT))\n",
        "\n",
        "        elif self.model_type == \"mlp\" and mlp_dir is not None:\n",
        "            saver = tf.compat.v1.train.Saver()\n",
        "            saver.restore(self.sess, os.path.join(mlp_dir, MODEL_CHECKPOINT))\n",
        "\n",
        "        elif self.model_type == \"neumf\" and neumf_dir is not None:\n",
        "            saver = tf.compat.v1.train.Saver()\n",
        "            saver.restore(self.sess, os.path.join(neumf_dir, MODEL_CHECKPOINT))\n",
        "\n",
        "        elif self.model_type == \"neumf\" and gmf_dir is not None and mlp_dir is not None:\n",
        "            # load neumf using gmf and mlp\n",
        "            self._load_neumf(gmf_dir, mlp_dir, alpha)\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def _load_neumf(self, gmf_dir, mlp_dir, alpha):\n",
        "        \"\"\"Load gmf and mlp model parameters for further use in NeuMF.\n",
        "        NeuMF model --> load parameters in `gmf_dir` and `mlp_dir`\n",
        "        \"\"\"\n",
        "        # load gmf part\n",
        "        variables = tf.compat.v1.global_variables()\n",
        "        # get variables with 'gmf'\n",
        "        var_flow_restore = [\n",
        "            val for val in variables if \"gmf\" in val.name and \"ncf\" not in val.name\n",
        "        ]\n",
        "        # load 'gmf' variable\n",
        "        saver = tf.compat.v1.train.Saver(var_flow_restore)\n",
        "        # restore\n",
        "        saver.restore(self.sess, os.path.join(gmf_dir, MODEL_CHECKPOINT))\n",
        "\n",
        "        # load mlp part\n",
        "        variables = tf.compat.v1.global_variables()\n",
        "        # get variables with 'gmf'\n",
        "        var_flow_restore = [\n",
        "            val for val in variables if \"mlp\" in val.name and \"ncf\" not in val.name\n",
        "        ]\n",
        "        # load 'gmf' variable\n",
        "        saver = tf.compat.v1.train.Saver(var_flow_restore)\n",
        "        # restore\n",
        "        saver.restore(self.sess, os.path.join(mlp_dir, MODEL_CHECKPOINT))\n",
        "\n",
        "        # concat pretrain h_from_gmf and h_from_mlp\n",
        "        vars_list = tf.compat.v1.get_collection(\n",
        "            tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=\"ncf\"\n",
        "        )\n",
        "\n",
        "        assert len(vars_list) == 1\n",
        "        ncf_fc = vars_list[0]\n",
        "\n",
        "        # get weight from gmf and mlp\n",
        "        gmf_fc = tf.train.load_variable(gmf_dir, ncf_fc.name)\n",
        "        mlp_fc = tf.train.load_variable(mlp_dir, ncf_fc.name)\n",
        "\n",
        "        # load fc layer by tf.concat\n",
        "        assign_op = tf.compat.v1.assign(\n",
        "            ncf_fc, tf.concat([alpha * gmf_fc, (1 - alpha) * mlp_fc], axis=0)\n",
        "        )\n",
        "        self.sess.run(assign_op)\n",
        "\n",
        "    def fit(self, data):\n",
        "        \"\"\"Fit model with training data\n",
        "        Args:\n",
        "            data (NCFDataset): initilized Dataset in ./dataset.py\n",
        "        \"\"\"\n",
        "\n",
        "        # get user and item mapping dict\n",
        "        self.user2id = data.user2id\n",
        "        self.item2id = data.item2id\n",
        "        self.id2user = data.id2user\n",
        "        self.id2item = data.id2item\n",
        "\n",
        "        # loop for n_epochs\n",
        "        for epoch_count in range(1, self.n_epochs + 1):\n",
        "\n",
        "            # negative sampling for training\n",
        "            train_begin = time()\n",
        "\n",
        "            # initialize\n",
        "            train_loss = []\n",
        "\n",
        "            # calculate loss and update NCF parameters\n",
        "            for user_input, item_input, labels in data.train_loader(self.batch_size):\n",
        "\n",
        "                user_input = np.array([self.user2id[x] for x in user_input])\n",
        "                item_input = np.array([self.item2id[x] for x in item_input])\n",
        "                labels = np.array(labels)\n",
        "\n",
        "                feed_dict = {\n",
        "                    self.user_input: user_input[..., None],\n",
        "                    self.item_input: item_input[..., None],\n",
        "                    self.labels: labels[..., None],\n",
        "                }\n",
        "\n",
        "                # get loss and execute optimization\n",
        "                loss, _ = self.sess.run([self.loss, self.optimizer], feed_dict)\n",
        "                train_loss.append(loss)\n",
        "            train_time = time() - train_begin\n",
        "\n",
        "            # output every self.verbose\n",
        "            if self.verbose and epoch_count % self.verbose == 0:\n",
        "                logger.info(\n",
        "                    \"Epoch %d [%.2fs]: train_loss = %.6f \"\n",
        "                    % (epoch_count, train_time, sum(train_loss) / len(train_loss))\n",
        "                )\n",
        "\n",
        "    def predict(self, user_input, item_input, is_list=False):\n",
        "        \"\"\"Predict function of this trained model\n",
        "        Args:\n",
        "            user_input (list or element of list): userID or userID list\n",
        "            item_input (list or element of list): itemID or itemID list\n",
        "            is_list (bool): if true, the input is list type\n",
        "                noting that list-wise type prediction is faster than element-wise's.\n",
        "        Returns:\n",
        "            list or float: A list of predicted rating or predicted rating score.\n",
        "        \"\"\"\n",
        "\n",
        "        if is_list:\n",
        "            output = self._predict(user_input, item_input)\n",
        "            return list(output.reshape(-1))\n",
        "\n",
        "        else:\n",
        "            output = self._predict(np.array([user_input]), np.array([item_input]))\n",
        "            return float(output.reshape(-1)[0])\n",
        "\n",
        "    def _predict(self, user_input, item_input):\n",
        "\n",
        "        # index converting\n",
        "        user_input = np.array([self.user2id[x] for x in user_input])\n",
        "        item_input = np.array([self.item2id[x] for x in item_input])\n",
        "\n",
        "        # get feed dict\n",
        "        feed_dict = {\n",
        "            self.user_input: user_input[..., None],\n",
        "            self.item_input: item_input[..., None],\n",
        "        }\n",
        "\n",
        "        # calculate predicted score\n",
        "        return self.sess.run(self.output, feed_dict)"
      ],
      "metadata": {
        "id": "pZ2Vn_fhjFgK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top k items to recommend\n",
        "TOP_K = 50\n",
        "\n",
        "# Model parameters\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "9TbveKS_jSDb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(ratings, test_size= 0.25)"
      ],
      "metadata": {
        "id": "Fic1vQ8lkdwt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.sort_values(by='userId')\n",
        "test = test.sort_values(by='userId')\n",
        "\n",
        "test = test[test[\"userId\"].isin(train[\"userId\"].unique())]\n",
        "test = test[test[\"movieId\"].isin(train[\"movieId\"].unique())]"
      ],
      "metadata": {
        "id": "zFE5WQDukjlf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = fpath+ \"./train.csv\"\n",
        "test_file = fpath+ \"./test.csv\"\n",
        "train.to_csv(train_file, index=False)\n",
        "test.to_csv(test_file, index=False)"
      ],
      "metadata": {
        "id": "ivu14rFFk0s5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset(train_file=train_file, test_file=test_file, seed=SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuJmp1OGk68H",
        "outputId": "7fd5ab20-f1a8-4adc-84f5-429deb29989c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Indexing /content/gdrive/MyDrive/ml_20M/ml-20m/./train.csv ...\n",
            "INFO:__main__:Indexing /content/gdrive/MyDrive/ml_20M/ml-20m/./test.csv ...\n",
            "INFO:__main__:Creating full leave-one-out test file /content/gdrive/MyDrive/ml_20M/ml-20m/./test_full.csv ...\n",
            "100%|██████████| 138457/138457 [3:47:35<00:00, 10.14it/s]\n",
            "INFO:__main__:Indexing /content/gdrive/MyDrive/ml_20M/ml-20m/./test_full.csv ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NCF (\n",
        "    n_users=data.n_users, \n",
        "    n_items=data.n_items,\n",
        "    model_type=\"NeuMF\",\n",
        "    n_factors=4,\n",
        "    layer_sizes=[1024,512,256],\n",
        "    n_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=10,\n",
        "    seed=SEED\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "rdgY8Z8jlKhE",
        "outputId": "fdfb0972-4e64-4eb1-b78c-303733463307"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-78dd8be8faeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = NCF (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mn_users\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_users\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mn_items\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NeuMF\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_factors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NCF' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z2r8hwnPSCG9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Colaboratory chào mừng bạn!",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}